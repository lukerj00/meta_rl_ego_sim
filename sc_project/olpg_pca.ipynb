{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scripts for general pca analysis of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_arr.shape= (1000, 60) rp_arr.shape= (1000, 60) sample_arr.shape= (1000, 60) mask_arr.shape= (1000, 60) \n",
      " pos_plan_arr.shape= (1000, 61, 2) pos_arr.shape= (1000, 61, 2) dot_arr.shape= (1000, 61, 2) policy_arr.shape= (1000, 60, 81) \n",
      " hs_arr= (1000, 60, 100) hv_arr= (1000, 60, 300)\n",
      "r_tot= 20.88939\n",
      "plan_rate= 0.08033241\n"
     ]
    }
   ],
   "source": [
    "### load arrs, pgv4 new\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "import jax.random as rnd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=1000, linewidth=200)\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.patches import Ellipse\n",
    "import os\n",
    "import functools\n",
    "\n",
    "def load_large_outputs(str_):\n",
    "    path_ = os.path.dirname(os.getcwd()) + '/sc_project/large_outputs/' # '/sc_project/test_data/' # Path(__file__).resolve().parents[1]\n",
    "    with open(path_+str_,'rb') as file_:\n",
    "        # param = pickle.load(file_)\n",
    "        param_ = jnp.load(file_,allow_pickle=True)\n",
    "    return param_\n",
    "\n",
    "def freeze_array(BA, P):\n",
    "    P2 = np.zeros_like(P)\n",
    "    P2[:, 0] = P[:, 0]\n",
    "    for t in range(1, P.shape[1]):\n",
    "        if BA[t] == 0:\n",
    "            P2[:, t] = P[:, t]  # Update to current value in P\n",
    "        else:\n",
    "            P2[:, t] = P2[:, t - 1]  # Hold previous value\n",
    "    return P2\n",
    "\n",
    "def compare_arrays(pos_arr, pos_plan_arr):\n",
    "    return np.where(pos_arr == pos_plan_arr, 0, 1)\n",
    "\n",
    "file_ = 'outer_loop_pg_new_v4f_ttg_26_11-115931.pkl' # 'outer_loop_pg_new_v4f_test_02_11-183526.pkl' # 'outer_loop_pg_new_v4f_23_10-231650.pkl'\n",
    "\n",
    "other = load_large_outputs(file_)\n",
    "\n",
    "## if not test:\n",
    "(selected_other,plan_info) = other\n",
    "(r_arr,rp_arr,sample_arr,mask_arr,pos_plan_arr,pos_arr,dot_arr,policy_arr,hs_arr,hv_arr,vec_ind_arr,act_ind_arr),_ = selected_other\n",
    "# plan:\n",
    "# (r_arr,rp_arr,sample_arr,mask_arr,pos_plan_arr,pos_arr,dot_arr,policy_arr,hs_arr,hv_arr,vec_ind_arr,act_ind_arr),_ = plan_info['other_']\n",
    "\n",
    "# if test:\n",
    "# (r_arr,rp_arr,sample_arr,mask_arr,pos_plan_arr,pos_arr,dot_arr,policy_arr,hs_arr,hv_arr,vec_ind_arr,act_ind_arr) = other\n",
    "print('r_arr.shape=',r_arr.shape,'rp_arr.shape=',rp_arr.shape,'sample_arr.shape=',sample_arr.shape,'mask_arr.shape=',mask_arr.shape,'\\n','pos_plan_arr.shape=',pos_plan_arr.shape,'pos_arr.shape=',pos_arr.shape,'dot_arr.shape=',dot_arr.shape,'policy_arr.shape=',policy_arr[0].shape,'\\n','hs_arr=',hs_arr.shape,'hv_arr=',hv_arr.shape)\n",
    "print('r_tot=',np.mean(np.sum(r_arr,axis=1)))\n",
    "print('plan_rate=',np.sum(sample_arr,axis=None)/np.sum(mask_arr,axis=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_arr_sorted = np.argsort(np.sum(sample_arr,axis=1))[::-1]\n",
    "print('sample_arr_sorted=',sample_arr_sorted[:10])\n",
    "\n",
    "print(sample_arr[615])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binned sort\n",
    "%matplotlib inline\n",
    "\n",
    "# Calculate dot speeds\n",
    "dot_diffs = np.diff(dot_arr, axis=1)\n",
    "dot_speeds = np.linalg.norm(dot_diffs, axis=2)\n",
    "mean_dot_speeds = np.mean(dot_speeds, axis=1)\n",
    "\n",
    "# Define speed bins\n",
    "speed_bins = np.linspace(np.min(mean_dot_speeds), np.max(mean_dot_speeds), num=5)  # Assuming 4 bins\n",
    "speed_bin_centers = (speed_bins[:-1] + speed_bins[1:]) / 2\n",
    "\n",
    "# Loop through speed bins\n",
    "for i in range(len(speed_bins) - 1):\n",
    "    mask = (mean_dot_speeds >= speed_bins[i]) & (mean_dot_speeds < speed_bins[i + 1])\n",
    "    \n",
    "    masked_indices = np.where(mask)[0]  # Keep track of original indices\n",
    "    masked_decisions = sample_arr[mask]\n",
    "    masked_rewards = r_arr[mask]\n",
    "    \n",
    "    total_rewards = np.sum(masked_rewards, axis=1)\n",
    "    planning_rates = np.mean(masked_decisions, axis=1)\n",
    "    product_scores = total_rewards * planning_rates\n",
    "\n",
    "    # Get the sorted indices in descending order\n",
    "    reward_indices = np.argsort(total_rewards)[::-1]\n",
    "    planning_indices = np.argsort(planning_rates)[::-1]\n",
    "    product_indices = np.argsort(product_scores)[::-1]\n",
    "\n",
    "    # Top 5 indices for rewards and planning rates\n",
    "    top_reward_indices = reward_indices[:5]\n",
    "    top_planning_indices = planning_indices[:50]\n",
    "    top_product_indices = product_indices[:10]\n",
    "    \n",
    "    # Map back to original indices\n",
    "    original_top_reward_indices = masked_indices[top_reward_indices]\n",
    "    original_top_planning_indices = masked_indices[top_planning_indices]\n",
    "    original_top_product_indices = masked_indices[top_product_indices]\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"top rewards for speed bin\", i+1)\n",
    "    for idx, original_idx in enumerate(original_top_reward_indices):\n",
    "        print(f\" Index: {original_idx}, Reward: {total_rewards[top_reward_indices[idx]]:.2f}\")\n",
    "\n",
    "    print(\"\\n top plan rates for speed bin\", i+1)\n",
    "    for idx, original_idx in enumerate(original_top_planning_indices):\n",
    "        print(f\"Index: {original_idx}, Planning Rate: {planning_rates[top_planning_indices[idx]]:.2f}, reward: {np.sum(r_arr[original_idx,:]):.2f}\")\n",
    "\n",
    "    print(\"\\n top Product Scores for speed bin\", i+1)\n",
    "    for idx, original_idx in enumerate(original_top_product_indices):\n",
    "        print(f\"Index: {original_idx}, Score: {product_scores[top_product_indices[idx]]:.2f}\")\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full ani pgv2/3 with policy heatmap\n",
    "\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors\n",
    "from matplotlib.patches import Ellipse\n",
    "import functools\n",
    "import jax.numpy as jnp\n",
    "import jax.random as rnd\n",
    "\n",
    "def gen_sc(keys,MODULES,ACTION_SPACE,PLAN_SPACE):\n",
    "    index_range = jnp.arange(MODULES**2)\n",
    "    x = jnp.linspace(-PLAN_SPACE,PLAN_SPACE,MODULES)\n",
    "    y = jnp.linspace(-PLAN_SPACE,PLAN_SPACE,MODULES)[::-1]\n",
    "    xv,yv = jnp.meshgrid(x,y)\n",
    "    A_full = jnp.vstack([xv.flatten(),yv.flatten()])\n",
    "\n",
    "    inner_mask = (jnp.abs(xv) <= ACTION_SPACE) & (jnp.abs(yv) <= ACTION_SPACE)\n",
    "    A_inner_ind = index_range[inner_mask.flatten()]\n",
    "    A_outer_ind = index_range[~inner_mask.flatten()]\n",
    "    A_inner_perm = rnd.permutation(keys[0],A_inner_ind)\n",
    "    A_outer_perm = rnd.permutation(keys[1],A_outer_ind)\n",
    "    ID_ARR = jnp.concatenate((A_inner_perm,A_outer_perm),axis=0)\n",
    "\n",
    "    VEC_ARR = A_full[:,ID_ARR]\n",
    "    H1VEC_ARR = jnp.eye(MODULES**2) # [:,ID_ARR]\n",
    "    SC = (ID_ARR,VEC_ARR,H1VEC_ARR)\n",
    "    return SC #,prior_vec,zero_vec_index\n",
    "\n",
    "def mod_(x):\n",
    "    return (x + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "def gen_vectors(m, A): # modules/neurons, aperture\n",
    "    x = np.linspace(-(A-A/m), (A-A/m), m)\n",
    "    x_ = np.tile(x, (m, ))\n",
    "    y_ = np.repeat(np.flip(x), m)\n",
    "    return np.vstack([x_, y_])\n",
    "\n",
    "def circ_mean_var(v_pred,vec_range):\n",
    "    v_clamped = np.clip(v_pred,0,None)\n",
    "    x_y_coords = gen_vectors(np.int32(np.sqrt(len(v_clamped))),vec_range)\n",
    "    x = x_y_coords[0,:]\n",
    "    y = x_y_coords[1,:]\n",
    "    z_x = v_pred*(np.cos(x) + 1j*np.sin(x))\n",
    "    z_y = v_pred*(np.cos(y) + 1j*np.sin(y))\n",
    "    mean_x = np.angle(np.sum(z_x)/np.sum(v_clamped))\n",
    "    mean_y = np.angle(np.sum(z_y)/np.sum(v_clamped))\n",
    "    circular_var_x = 1 - np.abs(np.sum(z_x) / np.sum(v_clamped))\n",
    "    circular_var_y = 1 - np.abs(np.sum(z_y) / np.sum(v_clamped))\n",
    "    circular_cov_matrix = np.diag([circular_var_x, circular_var_y])\n",
    "    eigvals,eigvecs = np.linalg.eigh(circular_cov_matrix)\n",
    "    sigma_x,sigma_y = np.sqrt(eigvals)  # no Scale factor for visualization\n",
    "    return x_y_coords,x,y,mean_x,mean_y,sigma_x,sigma_y\n",
    "\n",
    "def set_axis_properties(ax,lim,labels,title=''):\n",
    "    ax.set_xlim(-lim,lim)\n",
    "    ax.set_ylim(-lim,lim)\n",
    "    ax.set_aspect('equal', 'box')  \n",
    "    ax.set_title(title)\n",
    "    ticks = np.linspace(-lim,lim,5)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "def get_aperture_points(agent_t,aperture,density):\n",
    "    x_horizontal = np.linspace(agent_t[0] - aperture, agent_t[0] + aperture, density)\n",
    "    y_top = np.full(density, agent_t[1] + aperture)\n",
    "    y_bottom = np.full(density, agent_t[1] - aperture)\n",
    "    y_vertical = np.linspace(agent_t[1] - aperture, agent_t[1] + aperture, density)\n",
    "    x_left = np.full(density, agent_t[0] - aperture)\n",
    "    x_right = np.full(density, agent_t[0] + aperture)\n",
    "    x_tot = np.concatenate([x_left, x_right, x_horizontal, x_horizontal])\n",
    "    y_tot = np.concatenate([y_vertical, y_vertical, y_bottom, y_top])\n",
    "    return x_tot,y_tot\n",
    "\n",
    "def is_point_in_ellipse(x,y,h,k,a,b):\n",
    "    ellipse_eq = ((x - h)**2)/(a ** 2) + ((y - k)**2)/(b**2)\n",
    "    return ellipse_eq <= 1\n",
    "\n",
    "def vonmises_fit(x_coords,y_coords,v_pred_vec):\n",
    "    scalar = 100\n",
    "    kappa_max = 2\n",
    "    integer_weights = np.int32(np.round(v_pred_vec*scalar))\n",
    "    x_samples = np.repeat(x_coords, integer_weights)\n",
    "    y_samples = np.repeat(y_coords, integer_weights)\n",
    "    kappa_x,loc_x,_ = scipy.stats.vonmises.fit(x_samples)\n",
    "    kappa_y,loc_y,_ = scipy.stats.vonmises.fit(y_samples)\n",
    "    kappa_x = np.min([kappa_x,kappa_max])\n",
    "    kappa_y = np.min([kappa_y,kappa_max])\n",
    "    log_numerator = kappa_x + kappa_y\n",
    "    log_denominator = 2 * np.log(2*np.pi) + np.log(scipy.special.i0(kappa_x)) + np.log(scipy.special.i0(kappa_y))\n",
    "    log_result = log_numerator - log_denominator\n",
    "    mean_magnitude = np.exp(log_result)\n",
    "    return kappa_x,loc_x,kappa_y,loc_y,mean_magnitude\n",
    "\n",
    "def animate(i,r_arr,dot_arr,pos_plan_arr,pos_arr,policy_arr_vec,policy_arr_act,VEC_ARR,dot,agent_plan,agent,aperture_points_plan,aperture_points,act_space_points_plan,act_space_points,policy_scatter,L):\n",
    "    \n",
    "    dot.set_data(mod_(dot_arr[i, 0]), mod_(dot_arr[i, 1]))\n",
    "    agent_plan.set_data(mod_(pos_plan_arr[i, 0]), mod_(pos_plan_arr[i, 1]))\n",
    "    agent.set_data(mod_(pos_arr[i, 0]), mod_(pos_arr[i, 1]))\n",
    "\n",
    "    if i == 0:\n",
    "        aperture_points.set_color('green')\n",
    "    else:\n",
    "        aperture_points.set_color('lightgrey')\n",
    "\n",
    "    lab = f'Frame={i}, Reward={r_arr[i]:.3f}' #, k={kappa_x:.3f},{kappa_y:.3f}'\n",
    "    L.get_texts()[0].set_text(lab)\n",
    "\n",
    "    policy_dot_sizes = np.sqrt(policy_arr_vec[i,:])\n",
    "    policy_color = cm.hot(colormap_norm(policy_arr_act[i,0]))\n",
    "\n",
    "    x_aperture_plan,y_aperture_plan = get_aperture_points(pos_plan_arr[i],APERTURE,DENSITY_DEFAULT)\n",
    "    aperture_points_plan.set_offsets(np.c_[mod_(x_aperture_plan), mod_(y_aperture_plan)])\n",
    "    x_aperture,y_aperture = get_aperture_points(pos_arr[i],APERTURE,DENSITY_DEFAULT)\n",
    "    aperture_points.set_offsets(np.c_[mod_(x_aperture), mod_(y_aperture)])\n",
    "    x_act_space_plan,y_act_space_plan = get_aperture_points(pos_plan_arr[i],ACTION_SPACE,DENSITY_ACT_PLAN)\n",
    "    act_space_points_plan.set_offsets(np.c_[mod_(x_act_space_plan), mod_(y_act_space_plan)])\n",
    "    x_act_space,y_act_space = get_aperture_points(pos_arr[i],ACTION_SPACE,DENSITY_ACT_PLAN)\n",
    "    act_space_points.set_offsets(np.c_[mod_(x_act_space), mod_(y_act_space)])\n",
    "\n",
    "    policy_scatter.set_offsets(mod_(pos_plan_arr[i, :] + VEC_ARR.T))\n",
    "    policy_scatter.set_sizes(POLICY_DOT_SIZE*policy_dot_sizes)\n",
    "    policy_scatter.set_facecolors(policy_color)\n",
    "    \n",
    "    return [dot,agent_plan,agent,aperture_points_plan,aperture_points,act_space_points_plan,act_space_points,policy_scatter,L]\n",
    "\n",
    "k = 493 # 68 # 902\n",
    "interval = 200 # 150\n",
    "APERTURE = (1/2)*np.pi # (3/5``)*np.pi # (1/2)*np.pi # (np.sqrt(2)/2)*np.pi # np.pi/2\n",
    "ACTION_FRAC = 1 # 1/2\n",
    "ACTION_SPACE = ACTION_FRAC*APERTURE # np.pi/4\n",
    "PLAN_FRAC_REL = 1 #3/2 # 3/2\n",
    "PLAN_SPACE = PLAN_FRAC_REL*ACTION_SPACE # np.pi/2\n",
    "STEPS = pos_arr.shape[1]\n",
    "MODULES = 9 #\n",
    "SIGMA_SCALE = 3\n",
    "POLICY_DOT_SIZE = 70 # 100\n",
    "\n",
    "ke = rnd.split(rnd.PRNGKey(0),10)\n",
    "(ID_ARR,VEC_ARR,H1VEC_ARR) = gen_sc(ke,MODULES,ACTION_SPACE,PLAN_SPACE)\n",
    "\n",
    "DENSITY_DEFAULT = 1000\n",
    "DENSITY_ACT_PLAN = 15 # 10\n",
    "\n",
    "r_arr_ = r_arr[k,:]\n",
    "dot_arr_ = dot_arr[k,:,:] # [STEPS,2]\n",
    "pos_plan_arr_ = pos_plan_arr[k,:,:] # [STEPS,2]\n",
    "pos_arr_ = pos_arr[k,:,:] # [STEPS,2]\n",
    "policy_arr_vec = policy_arr[0][k,:,:]\n",
    "policy_arr_act = policy_arr[1][k,:,:]\n",
    "\n",
    "# r_arr_ = r_arr_np[k,:]\n",
    "# dot_arr_ = dot_arr_np[k,:,:] # [STEPS,2]\n",
    "# pos_plan_arr_ = pos_plan_arr_np[k,:,:] # [STEPS,2]\n",
    "# pos_arr_ = pos_arr_np[k,:,:] # [STEPS,2]\n",
    "# policy_arr_vec = policy_arr_np[0][k,:,:]\n",
    "# policy_arr_act = policy_arr_np[1][k,:,:]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "gs = gridspec.GridSpec(3, 2, height_ratios=[1, 1, 1])\n",
    "colormap_norm = matplotlib.colors.Normalize(vmin=0, vmax=1)\n",
    "\n",
    "ax_ani = plt.subplot(gs[0:2, 0:2],aspect='equal')\n",
    "set_axis_properties(ax_ani,jnp.pi,[\"$-\\pi$\", \"$-\\pi/2$\", \"0\", \"$\\pi/2$\", \"$\\pi$\"])\n",
    "agent_plan, = ax_ani.plot(mod_(pos_plan_arr_[0, 0]), mod_(pos_plan_arr_[0, 1]), color='lightgrey', marker='+', markersize=12) #, label='agent')\n",
    "agent, = ax_ani.plot(mod_(pos_arr_[0, 0]), mod_(pos_arr_[0, 1]), 'k+', markersize=12) #, label='agent')\n",
    "dot, = ax_ani.plot(mod_(dot_arr_[0, 0]), mod_(dot_arr_[0, 1]), 'rx', markersize=10) #, label='dot')\n",
    "text_ani, = ax_ani.plot([], [], '', label=f'Frame=0, Reward=')\n",
    "\n",
    "aperture_points = ax_ani.scatter([], [], color='lightgrey', s=1)\n",
    "aperture_points_plan = ax_ani.scatter([], [], color='lightgrey', s=1)\n",
    "act_space_points = ax_ani.scatter([], [], color='lightgrey', s=1)\n",
    "act_space_points_plan = ax_ani.scatter([], [], color='lightgrey', s=1)\n",
    "\n",
    "policy_color_init = cm.hot(colormap_norm(policy_arr_act[0,0])) # Assuming policy_arr_ corresponds to policy[1]\n",
    "policy_dot_sizes_init = np.sqrt(policy_arr_vec[0, :])\n",
    "policy_scatter = ax_ani.scatter(VEC_ARR[0, :], VEC_ARR[1, :], s=POLICY_DOT_SIZE*policy_dot_sizes_init, alpha=1, c=policy_color_init)\n",
    "\n",
    "L=ax_ani.legend(loc='upper right')\n",
    "\n",
    "partial_animate = functools.partial(\n",
    "    animate,\n",
    "    r_arr=r_arr_,\n",
    "    dot_arr=dot_arr_, \n",
    "    pos_plan_arr=pos_plan_arr_,\n",
    "    pos_arr=pos_arr_, \n",
    "    policy_arr_vec=policy_arr_vec,\n",
    "    policy_arr_act=policy_arr_act,\n",
    "    VEC_ARR=VEC_ARR,\n",
    "    dot=dot, \n",
    "    agent_plan=agent_plan,\n",
    "    agent=agent, \n",
    "    aperture_points_plan=aperture_points_plan,\n",
    "    aperture_points=aperture_points,\n",
    "    act_space_points_plan=act_space_points_plan,\n",
    "    act_space_points=act_space_points,\n",
    "    policy_scatter=policy_scatter,\n",
    "    L=L,\n",
    "    )\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig, \n",
    "    func=partial_animate,\n",
    "    frames=STEPS, \n",
    "    blit=True, \n",
    "    interval=interval\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 60, 5)\n",
      "(1000, 60, 5)\n"
     ]
    }
   ],
   "source": [
    "# NEURON-FOCUSED (RIGHT SINGULAR VECTORS)\n",
    "# PCA using SVD; get low-dim trajectories of top K PC's\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "K = 5 # 10\n",
    "\n",
    "def pca_svd_method(data, K):\n",
    "    # Reshape the data\n",
    "    num_samples, T, H = data.shape\n",
    "    reshaped_data = data.reshape(num_samples*T, H)\n",
    "    \n",
    "    # Center the data\n",
    "    mean_data = np.mean(reshaped_data, axis=0)\n",
    "    centered_data = reshaped_data - mean_data\n",
    "    \n",
    "    # Compute the SVD\n",
    "    U, S, V = np.linalg.svd(centered_data, full_matrices=False)\n",
    "    \n",
    "    # Get the top K columns of V\n",
    "    V_topK = V.T[:, :K]\n",
    "    \n",
    "    # Project data onto the top K PCs\n",
    "    projected_data = centered_data @ V_topK\n",
    "    \n",
    "    # Reshape the projected data back to [1000, 60, K]\n",
    "    projected_data = projected_data.reshape(num_samples, T, K)\n",
    "    \n",
    "    return projected_data\n",
    "\n",
    "hs_pc_s = pca_svd_method(hs_arr, K)\n",
    "hv_pc_s = pca_svd_method(hv_arr, K)\n",
    "\n",
    "print(hs_pc_s.shape)  # Should print (1000, 60, 3)\n",
    "print(hv_pc_s.shape)  # Should print (1000, 60, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURON-FOCUSED\n",
    "# PCA using covar matrix; get low-dim trajectories of top K PC's\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "K = 3\n",
    "\n",
    "def pca_covariance_method(data, K):\n",
    "    # Reshape the data to stack all time points across all trials\n",
    "    num_samples, T, H = data.shape\n",
    "    reshaped_data = data.reshape(num_samples*T, H)\n",
    "\n",
    "    # Subtract the overall mean across trials and time points\n",
    "    mean_data = np.mean(reshaped_data, axis=0)\n",
    "    centered_data = reshaped_data - mean_data\n",
    "    \n",
    "    # Calculate covariance matrix\n",
    "    cov_matrix = np.cov(centered_data, rowvar=False)\n",
    "    \n",
    "    # Eigen decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    \n",
    "    # Sorting eigenvectors based on eigenvalues\n",
    "    idx = eigenvalues.argsort()[::-1]  # Sort in descending order\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Take the top K eigenvectors\n",
    "    top_eigenvectors = eigenvectors[:, :K]\n",
    "    \n",
    "    # Project data onto top K eigenvectors to get principal components\n",
    "    projected_data = np.array([np.dot(data[i], top_eigenvectors) for i in range(data.shape[0])])\n",
    "    \n",
    "    return projected_data\n",
    "\n",
    "hs_pc_c = pca_covariance_method(hs_arr, K)\n",
    "hv_pc_c = pca_covariance_method(hv_arr, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Assuming 'full_activity' is your original data array of shape (1000, 60, 100)\n",
    "# Reshape the data for PCA: combining trials and timesteps, resulting in a (1000*60, 100) array\n",
    "full_activity_reshaped = hs_arr.reshape(-1, hs_arr.shape[2])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=20)  # Adjust the number of components as needed\n",
    "pca.fit(full_activity_reshaped)\n",
    "\n",
    "# Get the explained variance ratio of each component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(explained_variance)), explained_variance, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(len(cumulative_variance)), cumulative_variance, where='mid', label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.xticks(range(len(explained_variance)))\n",
    "plt.legend(loc='best')\n",
    "plt.title('Variance Explained by Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_param=np.linalg.norm(np.mean(np.diff(pos_arr,axis=1),axis=1),axis=1)\n",
    "print('task_param=',task_param.shape,task_param[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed decoding performance (single split)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming pca_data is your PCA-transformed data of shape (1000, 60, number_of_PCs)\n",
    "# Assuming task_param is your task parameter vector of length 1000\n",
    "\n",
    "max_timesteps = 60  # Maximum number of timesteps\n",
    "pcs_scenarios = [1, 3, 5]  # Top 1, top 5, and top 10 PCs\n",
    "rmses = {scenario: [] for scenario in pcs_scenarios}\n",
    "\n",
    "# Calculate the mean baseline RMSE\n",
    "mean_task_param = np.mean(task_param)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(task_param, np.full_like(task_param, mean_task_param)))\n",
    "\n",
    "for T in range(1, max_timesteps + 1):\n",
    "    for pcs in pcs_scenarios:\n",
    "        # Select the first T timesteps and first 'pcs' PCs\n",
    "        X_flattened = hs_pc_s[:, :T, :pcs].reshape(-1, T * pcs)\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_flattened, task_param, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train the model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set and calculate RMSE\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        rmses[pcs].append(rmse)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.axhline(y=baseline_rmse, color='r', linestyle='-', label='Mean Baseline')\n",
    "for pcs, rmse_values in rmses.items():\n",
    "    plt.plot(range(1, max_timesteps + 1), rmse_values, marker='o', label=f'Top {pcs} PCs')\n",
    "\n",
    "plt.xlabel('Number of Timesteps (T)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Decoding Performance with Varying Timesteps and PCs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no pcs\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Assuming pca_data is your PCA-transformed data of shape (1000, 60, 20)\n",
    "# And task_param is your task parameter vector of length 1000\n",
    "\n",
    "pca_data = hs_pc_s\n",
    "max_pcs = 10  # Maximum number of PCs\n",
    "rmses = []\n",
    "sem_values = []  # To store SEM values for the model\n",
    "random_rmses = []  # To store RMSEs for random baseline\n",
    "n_splits = 5  # Number of folds for K-Fold Cross-Validation\n",
    "\n",
    "task_param = np.linalg.norm(np.mean(np.diff(pos_arr, axis=1), axis=1), axis=1)\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to generate random predictions\n",
    "def generate_random_predictions(size):\n",
    "    return np.random.choice(task_param, size=size)\n",
    "\n",
    "# Calculate RMSE and SEM for random baseline across folds\n",
    "for train_index, test_index in kf.split(task_param):\n",
    "    y_test = task_param[test_index]\n",
    "    random_preds = generate_random_predictions(size=len(test_index))\n",
    "    random_rmses.append(np.sqrt(mean_squared_error(y_test, random_preds)))\n",
    "\n",
    "baseline_rmse = np.mean(random_rmses)\n",
    "baseline_sem = sem(random_rmses)\n",
    "\n",
    "# Model performance calculation\n",
    "for k in range(1, max_pcs + 1):\n",
    "    fold_rmses = []\n",
    "    X_flattened = pca_data[:, :, :k].reshape(-1, 60*k)\n",
    "\n",
    "    for train_index, test_index in kf.split(X_flattened):\n",
    "        X_train, X_test = X_flattened[train_index], X_flattened[test_index]\n",
    "        y_train, y_test = task_param[train_index], task_param[test_index]\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        fold_rmses.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    rmses.append(np.mean(fold_rmses))\n",
    "    sem_values.append(sem(fold_rmses))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.axhline(y=baseline_rmse, color='red', linestyle='-', label='Random Baseline')\n",
    "plt.fill_between(range(1, max_pcs + 1), [baseline_rmse - baseline_sem]*max_pcs, \n",
    "                 [baseline_rmse + baseline_sem]*max_pcs, color='lightgrey', alpha=0.5, label='Baseline SEM')\n",
    "for k in range(1, max_pcs + 1):\n",
    "    plt.plot(range(1, max_pcs + 1), rmses, marker='o', label='Model RMSE')\n",
    "    plt.fill_between(range(1, max_pcs + 1), np.array(rmses) - np.array(sem_values), \n",
    "                     np.array(rmses) + np.array(sem_values), color='lightgrey', alpha=0.5, label='Model SEM')\n",
    "\n",
    "plt.xlabel('Number of Principal Components Used')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs. Number of Principal Components')\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no pcs multi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Assuming pca_data is your PCA-transformed data of shape (1000, 60, 20)\n",
    "# Assuming pos_arr is defined and used to calculate task_param\n",
    "\n",
    "# Calculate task_param assuming pos_arr is correctly defined\n",
    "task_param = np.mean(np.diff(pos_arr, axis=1), axis=1)  # Check if this is the correct way to calculate task_param\n",
    "\n",
    "pca_data = hs_pc_s\n",
    "max_pcs = 10  # Maximum number of PCs\n",
    "n_splits = 5  # Number of folds for K-Fold Cross-Validation\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to generate random predictions for multi-output regression\n",
    "def generate_random_predictions(size, num_outputs):\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=(size, num_outputs))  # Adjust range as per your data\n",
    "\n",
    "# Initialize lists to store RMSEs and SEMs for each output\n",
    "num_outputs = task_param.shape[1]\n",
    "rmses = np.zeros((max_pcs, num_outputs))\n",
    "sem_values = np.zeros((max_pcs, num_outputs))\n",
    "random_rmses = []\n",
    "\n",
    "# Calculate RMSE for random baseline across folds\n",
    "for train_index, test_index in kf.split(pca_data):\n",
    "    y_test = task_param[test_index]\n",
    "    random_preds = generate_random_predictions(len(test_index), num_outputs)\n",
    "    random_rmses.append(np.sqrt(mean_squared_error(y_test, random_preds, multioutput='raw_values')))\n",
    "\n",
    "baseline_rmse = np.mean(random_rmses, axis=0)\n",
    "baseline_sem = sem(random_rmses, axis=0)\n",
    "\n",
    "# Model performance calculation for each number of PCs\n",
    "for k in range(max_pcs):\n",
    "    fold_rmses = []\n",
    "    X_flattened = pca_data[:, :, :k+1].reshape(-1, 60 * (k+1))\n",
    "\n",
    "    for train_index, test_index in kf.split(X_flattened):\n",
    "        X_train, X_test = X_flattened[train_index], X_flattened[test_index]\n",
    "        y_train, y_test = task_param[train_index], task_param[test_index]\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        fold_rmses.append(np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values')))\n",
    "\n",
    "    rmses[k, :] = np.mean(fold_rmses, axis=0)\n",
    "    sem_values[k, :] = sem(fold_rmses, axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(num_outputs):\n",
    "    plt.plot(range(1, max_pcs + 1), rmses[:, i], marker='o', label=f'Output {i+1} RMSE')\n",
    "    plt.fill_between(range(1, max_pcs + 1), rmses[:, i] - sem_values[:, i], rmses[:, i] + sem_values[:, i], alpha=0.5)\n",
    "\n",
    "# Baseline plots\n",
    "for i in range(num_outputs):\n",
    "    plt.axhline(y=baseline_rmse[i], linestyle='-', label=f'Random Baseline {i+1}') \n",
    "    plt.fill_between(range(1, max_pcs + 1), [baseline_rmse[i] - baseline_sem[i]] * max_pcs, \n",
    "                     [baseline_rmse[i] + baseline_sem[i]] * max_pcs, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Number of Principal Components Used')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs. Number of Principal Components for Each Output')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # speed k fold\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Assuming pca_data is your PCA-transformed data of shape (1000, 60, number_of_PCs)\n",
    "# Assuming task_param is your task parameter vector of length 1000\n",
    "\n",
    "max_timesteps = 60  # Maximum number of timesteps\n",
    "pcs_scenarios = [1, 3, 5]  # Top 1, top 5, and top 10 PCs\n",
    "rmses = {scenario: [] for scenario in pcs_scenarios}\n",
    "sems = {scenario: [] for scenario in pcs_scenarios}\n",
    "n_splits = 5  # Number of folds in cross-validation\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "clrs=['#ADD8E6','#1E90FF','#00008B']\n",
    "\n",
    "# Function to generate random predictions\n",
    "def generate_random_predictions(y, num_samples):\n",
    "    return np.random.choice(y, size=num_samples)\n",
    "\n",
    "# Calculate RMSE and SEM for random baseline\n",
    "random_rmse = []\n",
    "for _ in range(1000):  # Bootstrap iterations\n",
    "    random_preds = generate_random_predictions(task_param, len(task_param))\n",
    "    random_rmse.append(np.sqrt(mean_squared_error(task_param, random_preds)))\n",
    "baseline_rmse = np.mean(random_rmse)\n",
    "baseline_sem = sem(random_rmse)\n",
    "\n",
    "for T in range(1, max_timesteps + 1):\n",
    "    for pcs in pcs_scenarios:\n",
    "        rmse_fold = []\n",
    "        X_flattened = hs_pc_s[:, :T, :pcs].reshape(-1, T * pcs)\n",
    "\n",
    "        for train_index, test_index in kf.split(X_flattened):\n",
    "            X_train, X_test = X_flattened[train_index], X_flattened[test_index]\n",
    "            y_train, y_test = task_param[train_index], task_param[test_index]\n",
    "\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse_fold.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "        rmses[pcs].append(np.mean(rmse_fold))\n",
    "        sems[pcs].append(sem(rmse_fold))\n",
    "\n",
    "# Plotting\n",
    "plw=6\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.axhline(y=baseline_rmse, color='r', linestyle='-', label='random')\n",
    "plt.fill_between(range(1, max_timesteps + 1), [baseline_rmse - baseline_sem] * max_timesteps, \n",
    "                 [baseline_rmse + baseline_sem] * max_timesteps, color='red', alpha=0.2)  # Baseline SEM shading\n",
    "\n",
    "for (pcs, rmse_values), color in zip(rmses.items(), clrs):\n",
    "    plt.plot(range(1, max_timesteps + 1), rmse_values, label=f'{pcs} PCs', color=color, linewidth=plw)\n",
    "    plt.fill_between(range(1, max_timesteps + 1), \n",
    "                     np.array(rmse_values) - np.array(sems[pcs]), \n",
    "                     np.array(rmse_values) + np.array(sems[pcs]), \n",
    "                     alpha=0.5, color='lightgrey')  # Model SEM shading\n",
    "    \n",
    "lw = 4\n",
    "fnt = 42\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(lw)\n",
    "ax.spines['bottom'].set_linewidth(lw)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fnt, length=10, width=lw)  # Adjust length and width as needed\n",
    "ax.set_xticks([0, 30, 60])\n",
    "ax.set_yticks([0.05, 0.09, 0.13])\n",
    "\n",
    "\n",
    "plt.xlabel('$\\#$ timesteps', fontsize=fnt, labelpad=0)\n",
    "plt.ylabel('RMSE', fontsize=fnt, labelpad=0)\n",
    "# plt.title('Decoding Performance with Varying Timesteps and PCs')\n",
    "plt.legend(loc='center right', ncols =1, fontsize=30, frameon=False, bbox_to_anchor=(1.0, 0.5))\n",
    "# plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('baseline_sem=',baseline_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-output k fold\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming pca_data is your PCA-transformed data of shape (1000, 60, number_of_PCs)\n",
    "# Assuming task_param is your task parameter matrix of shape (1000, 2)\n",
    "\n",
    "clrs=['#ADD8E6','#1E90FF','#00008B']\n",
    "\n",
    "# Function to generate random predictions for multi-output regression\n",
    "def generate_random_predictions(num_samples, num_targets, target_ranges):\n",
    "    # 'target_ranges' should be a list of arrays, where each array is the range of possible values for each target\n",
    "    return np.array([np.random.choice(target_ranges[i], size=num_samples) for i in range(num_targets)]).T\n",
    "\n",
    "task_param=np.mean(np.diff(pos_arr,axis=1),axis=1)\n",
    "\n",
    "# Calculate RMSE for random baseline\n",
    "baseline_rmses = []\n",
    "target_ranges = [np.unique(task_param[:, i]) for i in range(task_param.shape[1])]  # Assuming task_param is 2D\n",
    "\n",
    "for T in range(1, max_timesteps + 1):\n",
    "    random_preds = generate_random_predictions(len(task_param), task_param.shape[1], target_ranges=target_ranges)\n",
    "    baseline_rmse_output1 = np.sqrt(mean_squared_error(task_param[:, 0], random_preds[:, 0]))\n",
    "    baseline_rmse_output2 = np.sqrt(mean_squared_error(task_param[:, 1], random_preds[:, 1]))\n",
    "    baseline_rmses.append((baseline_rmse_output1 + baseline_rmse_output2) / 2)\n",
    "\n",
    "max_timesteps = 60  # Maximum number of timesteps\n",
    "pcs_scenarios = [1, 3, 5]  # Top 1, top 5, and top 10 PCs\n",
    "rmses = {scenario: [] for scenario in pcs_scenarios}\n",
    "\n",
    "n_splits = 5  # Number of folds in cross-validation\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "for T in range(1, max_timesteps + 1):\n",
    "    for pcs in pcs_scenarios:\n",
    "        rmse_fold = []\n",
    "        X_flattened = hs_pc_s[:, :T, :pcs].reshape(-1, T * pcs)\n",
    "\n",
    "        for train_index, test_index in kf.split(X_flattened):\n",
    "            X_train, X_test = X_flattened[train_index], X_flattened[test_index]\n",
    "            y_train, y_test = task_param[train_index], task_param[test_index]\n",
    "\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Calculate RMSE for each output and average\n",
    "            rmse_output1 = np.sqrt(mean_squared_error(y_test[:, 0], y_pred[:, 0]))\n",
    "            rmse_output2 = np.sqrt(mean_squared_error(y_test[:, 1], y_pred[:, 1]))\n",
    "            rmse_fold.append((rmse_output1 + rmse_output2) / 2)\n",
    "\n",
    "        rmses[pcs].append(np.mean(rmse_fold))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))  # Increased figure size for clarity\n",
    "for pcs, rmse_values in rmses.items():\n",
    "    plt.plot(range(1, max_timesteps + 1), rmse_values, marker='o', label=f'Top {pcs} PCs', linewidth=2)\n",
    "\n",
    "# Plot baseline RMSE\n",
    "plt.plot(range(1, max_timesteps + 1), baseline_rmses, color='red', linestyle='--', label='Random Baseline', linewidth=2)\n",
    "\n",
    "plt.xlabel('Number of Timesteps (T)', fontsize=14)\n",
    "plt.ylabel('Average RMSE', fontsize=14)\n",
    "plt.title('Multi-output Decoding Performance', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 00:19:12.211 Python[96371:10875106] +[CATransaction synchronize] called within transaction\n"
     ]
    }
   ],
   "source": [
    "# # multi output k fold sem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import sem\n",
    "\n",
    "# clrs=['#ADD8E6','#1E90FF','#00008B']\n",
    "\n",
    "# def generate_random_predictions(num_samples, num_targets, target_ranges):\n",
    "#     return np.array([np.random.choice(target_ranges[i], size=num_samples) for i in range(num_targets)]).T\n",
    "\n",
    "# task_param=np.mean(np.diff(pos_arr,axis=1),axis=1)\n",
    "\n",
    "# baseline_rmses_output1 = []\n",
    "# baseline_rmses_output2 = []\n",
    "# target_ranges = [np.unique(task_param[:, i]) for i in range(task_param.shape[1])]  # Assuming task_param is 2D\n",
    "\n",
    "# for T in range(1, max_timesteps + 1):\n",
    "#     random_preds = generate_random_predictions(len(task_param), task_param.shape[1], target_ranges=target_ranges)\n",
    "#     baseline_rmse_output1 = np.sqrt(mean_squared_error(task_param[:, 0], random_preds[:, 0]))\n",
    "#     baseline_rmse_output2 = np.sqrt(mean_squared_error(task_param[:, 1], random_preds[:, 1]))\n",
    "#     baseline_rmses_output1.append(baseline_rmse_output1)\n",
    "#     baseline_rmses_output2.append(baseline_rmse_output2)\n",
    "\n",
    "# # Calculate combined baseline RMSE\n",
    "# combined_baseline_rmse = np.mean([np.mean(baseline_rmses_output1), np.mean(baseline_rmses_output2)])\n",
    "\n",
    "# # Calculate SEM for each output\n",
    "# sem_output1 = np.std(baseline_rmses_output1, ddof=1) / np.sqrt(len(baseline_rmses_output1))\n",
    "# sem_output2 = np.std(baseline_rmses_output2, ddof=1) / np.sqrt(len(baseline_rmses_output2))\n",
    "\n",
    "# # Optionally, calculate the combined SEM\n",
    "# combined_baseline_sem = np.mean([sem_output1, sem_output2])\n",
    "\n",
    "# max_timesteps = 60  # Maximum number of timesteps\n",
    "# pcs_scenarios = [1, 3, 5]  # Top 1, top 3, and top 5 PCs\n",
    "# rmses = {scenario: [] for scenario in pcs_scenarios}\n",
    "# sems = {scenario: [] for scenario in pcs_scenarios}  # Store SEMs here\n",
    "\n",
    "# n_splits = 5  # Number of folds in cross-validation\n",
    "# kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# for T in range(1, max_timesteps + 1):\n",
    "#     for pcs in pcs_scenarios:\n",
    "#         rmse_fold = []\n",
    "#         X_flattened = hs_pc_s[:, :T, :pcs].reshape(-1, T * pcs)\n",
    "\n",
    "#         for train_index, test_index in kf.split(X_flattened):\n",
    "#             X_train, X_test = X_flattened[train_index], X_flattened[test_index]\n",
    "#             y_train, y_test = task_param[train_index], task_param[test_index]\n",
    "\n",
    "#             model = LinearRegression()\n",
    "#             model.fit(X_train, y_train)\n",
    "#             y_pred = model.predict(X_test)\n",
    "\n",
    "#             rmse_output1 = np.sqrt(mean_squared_error(y_test[:, 0], y_pred[:, 0]))\n",
    "#             rmse_output2 = np.sqrt(mean_squared_error(y_test[:, 1], y_pred[:, 1]))\n",
    "#             rmse_fold.append((rmse_output1 + rmse_output2) / 2)\n",
    "\n",
    "#         rmses[pcs].append(np.mean(rmse_fold))\n",
    "#         sems[pcs].append(sem(rmse_fold))  # Calculate and append SEM\n",
    "\n",
    "# Plotting\n",
    "plw = 6\n",
    "plt.figure(figsize=(6.5, 6))\n",
    "\n",
    "# Plot a single line for the combined average random baseline RMSE and SEM\n",
    "plt.axhline(y=combined_baseline_rmse, color='r', linestyle='--', linewidth=6,label='random')\n",
    "plt.fill_between(range(1, max_timesteps + 1), \n",
    "                 [combined_baseline_rmse - combined_baseline_sem] * max_timesteps, \n",
    "                 [combined_baseline_rmse + combined_baseline_sem] * max_timesteps, \n",
    "                 color='lightgrey', alpha=0.2)\n",
    "\n",
    "# Plot RMSE and SEM for each number of PCs\n",
    "for (pcs, rmse_values), color in zip(rmses.items(), clrs):\n",
    "    if pcs == 1:\n",
    "        label_text = f'{pcs} PC'  # Singular\n",
    "    else:\n",
    "        label_text = f'{pcs} PCs'  # Plural\n",
    "\n",
    "    plt.plot(range(1, max_timesteps + 1), rmse_values, label=label_text, color=color, linewidth=plw)\n",
    "    plt.fill_between(range(1, max_timesteps + 1), \n",
    "                     np.array(rmse_values) - np.array(sems[pcs]), \n",
    "                     np.array(rmse_values) + np.array(sems[pcs]), \n",
    "                     alpha=0.5, color='lightgrey')\n",
    "\n",
    "# Styling axes and labels\n",
    "lw = 4\n",
    "fnt = 38\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(lw)\n",
    "ax.spines['bottom'].set_linewidth(lw)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fnt, length=10, width=lw)\n",
    "ax.set_xticks([0, 30, 60])\n",
    "ax.set_yticks([0,0.1,0.2])\n",
    "ax.set_ylim([0,0.24])\n",
    "\n",
    "plt.xlabel('$\\#$ timesteps', fontsize=fnt)\n",
    "plt.ylabel('RMSE', fontsize=fnt)\n",
    "plt.legend(loc='upper center', ncols=2, fontsize=28, frameon=False, handlelength=1.3, handletextpad=0.5, columnspacing=0.4, bbox_to_anchor=(0.53, 1.27))\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust this value as needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi output k fold sem (broken)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import sem\n",
    "\n",
    "clrs=['#ADD8E6','#1E90FF','#00008B']\n",
    "\n",
    "def generate_random_predictions(num_samples, num_targets, target_ranges):\n",
    "    return np.array([np.random.choice(target_ranges[i], size=num_samples) for i in range(num_targets)]).T\n",
    "\n",
    "task_param=np.mean(np.diff(pos_arr,axis=1),axis=1)\n",
    "\n",
    "target_ranges = [np.unique(task_param[:, i]) for i in range(task_param.shape[1])]  # Assuming task_param is 2D\n",
    "for T in range(1, max_timesteps + 1):\n",
    "    random_preds = generate_random_predictions(len(task_param), task_param.shape[1], target_ranges=target_ranges)\n",
    "    baseline_rmse_output1 = np.sqrt(mean_squared_error(task_param[:, 0], random_preds[:, 0]))\n",
    "    baseline_rmse_output2 = np.sqrt(mean_squared_error(task_param[:, 1], random_preds[:, 1]))\n",
    "    baseline_rmses.append((baseline_rmse_output1 + baseline_rmse_output2) / 2)\n",
    "\n",
    "baseline_rmse = np.mean(random_preds)\n",
    "baseline_sem = sem(baseline_rmses)\n",
    "\n",
    "combined_baseline_rmse = np.mean(baseline_rmse)\n",
    "combined_baseline_sem = np.mean(baseline_sem)\n",
    "\n",
    "max_timesteps = 60  # Maximum number of timesteps\n",
    "pcs_scenarios = [1, 3, 5]  # Top 1, top 3, and top 5 PCs\n",
    "rmses = {scenario: [] for scenario in pcs_scenarios}\n",
    "sems = {scenario: [] for scenario in pcs_scenarios}  # Store SEMs here\n",
    "\n",
    "n_splits = 5  # Number of folds in cross-validation\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "for T in range(1, max_timesteps + 1):\n",
    "    for pcs in pcs_scenarios:\n",
    "        rmse_fold = []\n",
    "        X_flattened = hs_pc_s[:, :T, :pcs].reshape(-1, T * pcs)\n",
    "\n",
    "        for train_index, test_index in kf.split(X_flattened):\n",
    "            X_train, X_test = X_flattened[train_index], X_flattened[test_index]\n",
    "            y_train, y_test = task_param[train_index], task_param[test_index]\n",
    "\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            rmse_output1 = np.sqrt(mean_squared_error(y_test[:, 0], y_pred[:, 0]))\n",
    "            rmse_output2 = np.sqrt(mean_squared_error(y_test[:, 1], y_pred[:, 1]))\n",
    "            rmse_fold.append((rmse_output1 + rmse_output2) / 2)\n",
    "\n",
    "        rmses[pcs].append(np.mean(rmse_fold))\n",
    "        sems[pcs].append(sem(rmse_fold))  # Calculate and append SEM\n",
    "\n",
    "# Plotting\n",
    "plw = 6\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "# Plot a single line for the combined average random baseline RMSE and SEM\n",
    "plt.axhline(y=combined_baseline_rmse, color='r', linestyle='--', linewidth=4,label='random')\n",
    "plt.fill_between(range(1, max_timesteps + 1), \n",
    "                 [combined_baseline_rmse - combined_baseline_sem] * max_timesteps, \n",
    "                 [combined_baseline_rmse + combined_baseline_sem] * max_timesteps, \n",
    "                 color='lightgrey', alpha=0.2)\n",
    "\n",
    "# Plot RMSE and SEM for each number of PCs\n",
    "for (pcs, rmse_values), color in zip(rmses.items(), clrs):\n",
    "    if pcs == 1:\n",
    "        label_text = f'{pcs} PC'  # Singular\n",
    "    else:\n",
    "        label_text = f'{pcs} PCs'  # Plural\n",
    "\n",
    "    plt.plot(range(1, max_timesteps + 1), rmse_values, label=label_text, color=color, linewidth=plw)\n",
    "    plt.fill_between(range(1, max_timesteps + 1), \n",
    "                     np.array(rmse_values) - np.array(sems[pcs]), \n",
    "                     np.array(rmse_values) + np.array(sems[pcs]), \n",
    "                     alpha=0.5, color='lightgrey')\n",
    "\n",
    "# Styling axes and labels\n",
    "lw = 4\n",
    "fnt = 38\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(lw)\n",
    "ax.spines['bottom'].set_linewidth(lw)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fnt, length=10, width=lw)\n",
    "ax.set_xticks([0, 30, 60])\n",
    "ax.set_yticks([0.6, 0.4, 0.2,0])\n",
    "\n",
    "plt.xlabel('$\\#$ timesteps', fontsize=fnt)\n",
    "plt.ylabel('RMSE', fontsize=fnt)\n",
    "plt.legend(loc='center right', ncols=1, fontsize=30, frameon=False, bbox_to_anchor=(1.0, 0.55), handlelength=1.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot specified trajectories; plans in black\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def plot_trajectories(fig, data, sample_arr, n_list):\n",
    "    colormaps = ['PiYG', 'PRGn', 'PuOr', 'RdBu']\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    for idx, n in enumerate(n_list):\n",
    "        traj = data[n, :, :3]\n",
    "        length = traj.shape[0]\n",
    "        \n",
    "        # Compute transitions in sample_arr for current trajectory\n",
    "        transitions = np.where(np.diff(sample_arr[n, :]) != 0)[0]\n",
    "        print('tr=',transitions)\n",
    "        \n",
    "        # Add start or end point if the trajectory starts or ends with '1'\n",
    "        if sample_arr[n, 0] == 1:\n",
    "            transitions = np.insert(transitions, 0, -1)\n",
    "        if sample_arr[n, -1] == 1:\n",
    "            transitions = np.append(transitions, length-2)\n",
    "        \n",
    "        # Create color array based on sample_arr\n",
    "        colors = np.array([plt.get_cmap(colormaps[idx])(i) if sample_arr[n, i] == 0 else (0, 0, 0, 1) for i in range(length)])\n",
    "        \n",
    "        segments_3d = [traj[i:i+2] for i in range(length-1)]\n",
    "        segments_2d = [s[:, :2] for s in segments_3d]\n",
    "        \n",
    "        lc_3d = Line3DCollection(segments_3d, colors=colors[:-1])\n",
    "        lc_2d = LineCollection(segments_2d, colors=colors[:-1])\n",
    "        \n",
    "        ax1.add_collection(lc_3d)\n",
    "        ax2.add_collection(lc_2d)\n",
    "\n",
    "        # Add dots for transitions\n",
    "        if sample_arr[n, 0] == 1:\n",
    "            ax1.scatter(*traj[0, :3], color='black', s=10)\n",
    "            ax2.scatter(*traj[0, :2], color='black', s=10)\n",
    "\n",
    "        if sample_arr[n, -1] == 1:\n",
    "            ax1.scatter(*traj[-1, :3], color='black', s=10)\n",
    "            ax2.scatter(*traj[-1, :2], color='black', s=10)\n",
    "\n",
    "        for t in transitions:\n",
    "            ax1.scatter(*traj[t+1, :3], color='black', s=10)\n",
    "            ax2.scatter(*traj[t+1, :2], color='black', s=10)\n",
    "\n",
    "    # Set limits based on data\n",
    "    all_data = data[:, :, :3].reshape(-1, 3)\n",
    "    ax1.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax1.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax1.set_zlim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "    ax2.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax2.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "\n",
    "    ax1.set_title('3D Trajectories')\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax2.set_title('2D Projections of Trajectories')\n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "n_list = [615, 616] # 133\n",
    "# [884, 87, 20] # R2\n",
    "# [335,796,479] # R1\n",
    "plot_trajectories(fig, hs_pc_s, sample_arr, n_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### colorwheel (correct)\n",
    "# top T trajectories binned by dot vector (avg/all)\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def generate_bin_vectors(N):\n",
    "    angles = np.linspace(0, 2 * np.pi, N+1)[:-1]  # Omitting the last angle to avoid overlap with 0\n",
    "    return np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "\n",
    "# def has_duplicates(array_2d):\n",
    "#     flat_list = array_2d.flatten().tolist()\n",
    "#     unique_set = set(flat_list)\n",
    "#     return len(unique_set) < len(flat_list)\n",
    "\n",
    "def generate_polar_angles(N):\n",
    "    step = 360 / N\n",
    "    angles = [(i * step) % 360 for i in range(N)]\n",
    "    return [f\"{int(angle)}\" if angle <= 180 else f\"{int(angle - 360)}\" for angle in angles]\n",
    "\n",
    "def compute_dot_direction(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_dir = np.mean(dot_diffs, axis=1)\n",
    "    norm_dot_dir = np.linalg.norm(mean_dot_dir, axis=1)\n",
    "    mean_dot_dir_normalized = mean_dot_dir / norm_dot_dir[:, np.newaxis]\n",
    "    return mean_dot_dir_normalized\n",
    "\n",
    "def get_top_rewarded_trials_new(dot_arr, r_arr, T, N):\n",
    "    bins = generate_bin_vectors(N)\n",
    "    normalized_dot_dirs = compute_dot_direction(dot_arr)\n",
    "\n",
    "    # Assign each dot to a bin\n",
    "    bin_assignments = []\n",
    "    for dot in normalized_dot_dirs:\n",
    "        cosine_similarity = dot @ np.array(bins).T\n",
    "        assigned_bin = np.argmax(cosine_similarity)\n",
    "        bin_assignments.append(assigned_bin)\n",
    "\n",
    "    # Organize trials by their assigned bin and then sort them by total reward\n",
    "    total_rewards = np.sum(r_arr, axis=1)\n",
    "    bin_lists = [[] for _ in range(N)]\n",
    "    for i, assigned_bin in enumerate(bin_assignments):\n",
    "        bin_lists[assigned_bin].append(i)\n",
    "    \n",
    "    # Sort trials in each bin by reward and select top T trials\n",
    "    top_trials = np.zeros((T, N), dtype=int)\n",
    "    for idx, trial_list in enumerate(bin_lists):\n",
    "        sorted_trials = sorted(trial_list, key=lambda x: total_rewards[x], reverse=True)\n",
    "        top_trials[:, idx] = sorted_trials[:T]\n",
    "\n",
    "    return top_trials\n",
    "\n",
    "def plot_trajectories(fig, data, top_trials, N):\n",
    "    # Define a color wheel\n",
    "    color_wheel = plt.cm.hsv(np.linspace(0, 1, N+1))\n",
    "    custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', color_wheel)\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    for bin_idx, trials in enumerate(top_trials.T): # Transpose to iterate over trials per bin\n",
    "        for trial_idx in trials:\n",
    "            traj = data[trial_idx, :, :3]\n",
    "            segments_3d = [traj[i:i+2] for i in range(traj.shape[0]-1)]\n",
    "            segments_2d = [s[:, :2] for s in segments_3d]\n",
    "            \n",
    "            lc_3d = Line3DCollection(segments_3d, colors=color_wheel[bin_idx])\n",
    "            lc_2d = LineCollection(segments_2d, colors=color_wheel[bin_idx])\n",
    "            \n",
    "            ax1.add_collection(lc_3d)\n",
    "            ax2.add_collection(lc_2d)\n",
    "    \n",
    "    # Set limits based on data\n",
    "    all_data = data[:, :, :3].reshape(-1, 3)\n",
    "    ax1.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax1.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax1.set_zlim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "    ax2.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax2.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "\n",
    "    polar_angles = generate_polar_angles(N) # [\"0\", \"45\", \"90\", \"135\", \"180\", \"-135\", \"-90\", \"-45\"]\n",
    "    legend_elements = [Line2D([0], [0], color=color_wheel[i], lw=2, label=polar_angles[i]) for i in range(N)]\n",
    "    ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "    \n",
    "    ax1.set_title('3D Trajectories')\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax2.set_title('2D Projections of Trajectories')\n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_avg_trajectories(fig, data, top_trials, N):\n",
    "    # Define a color wheel\n",
    "    color_wheel = plt.cm.hsv(np.linspace(0, 1, N+1))\n",
    "    custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', color_wheel)\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    # ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    for bin_idx, trials in enumerate(top_trials.T): \n",
    "        # Calculate the average trajectory for the trials in this bin\n",
    "        avg_traj = np.mean(data[trials], axis=0)\n",
    "        \n",
    "        segments_3d = [avg_traj[i:i+2] for i in range(avg_traj.shape[0]-1)]\n",
    "        segments_2d = [s[:, :2] for s in segments_3d]\n",
    "        \n",
    "        lc_3d = Line3DCollection(segments_3d, colors=color_wheel[bin_idx])\n",
    "        lc_2d = LineCollection(segments_2d, colors=color_wheel[bin_idx])\n",
    "        \n",
    "        ax1.add_collection(lc_3d)\n",
    "        # ax2.add_collection(lc_2d)\n",
    "    \n",
    "    top_data_indices = top_trials.flatten()  # Assuming top_trials is a 2D array of indices\n",
    "    top_data = data[top_data_indices, :, :3].reshape(-1, 3)\n",
    "\n",
    "    # Define padding as a percentage of the data range\n",
    "    padding_factor = -0.0  # 5% padding\n",
    "\n",
    "    # Calculate the limits based on the top_data with padding\n",
    "    x_padding = (top_data[:, 0].max() - top_data[:, 0].min()) * padding_factor\n",
    "    y_padding = (top_data[:, 1].max() - top_data[:, 1].min()) * padding_factor\n",
    "    z_padding = (top_data[:, 2].max() - top_data[:, 2].min()) * padding_factor\n",
    "\n",
    "    x_range = (top_data[:, 0].min() - x_padding, top_data[:, 0].max() + x_padding)\n",
    "    y_range = (top_data[:, 1].min() - y_padding, top_data[:, 1].max() + y_padding)\n",
    "    z_range = (top_data[:, 2].min() - z_padding, top_data[:, 2].max() + z_padding)\n",
    "\n",
    "    # Set the limits for each axis based on the top_data with padding\n",
    "    ax1.set_xlim(x_range)\n",
    "    ax1.set_ylim(y_range)\n",
    "    ax1.set_zlim(z_range)\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap=custom_cmap, norm=plt.Normalize(vmin=0, vmax=N))\n",
    "    sm.set_array([])  # You can set an array of values here if needed for scaling\n",
    "    cbar = plt.colorbar(sm, orientation='horizontal', ticks=[], aspect=4)\n",
    "    cbar.outline.set_edgecolor('none')  # Remove the colorbar border\n",
    "\n",
    "    # Manually adjust the size and position of the colorbar\n",
    "    cbar_width = ax1.get_position().width\n",
    "    cbar_height = 0.025  # Set the height of the colorbar\n",
    "    cbar_pos_x = ax1.get_position().x0\n",
    "    cbar_pos_y = ax1.get_position().y1 # + 0.05  # Adjust vertical position\n",
    "    cbar.ax.set_position([cbar_pos_x - 0.03, cbar_pos_y - .07, cbar_width, cbar_height])\n",
    "\n",
    "    # Remove tick marks from the colorbar\n",
    "    cbar.ax.xaxis.set_ticks_position('top')\n",
    "    cbar.ax.xaxis.set_tick_params(size=0)\n",
    "\n",
    "    # Add the 'time' label to the right\n",
    "    cbar.ax.text(1.2, 0.5, 'angle', verticalalignment='center', transform=cbar.ax.transAxes, fontsize=12)\n",
    "\n",
    "    # ax1.set_box_aspect([1,1,1]) \n",
    "    # ax2.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    # ax2.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    \n",
    "    # Create a legend with polar angles corresponding to the bins\n",
    "    polar_angles = generate_polar_angles(N) # [\"0\", \"45\", \"90\", \"135\", \"180\", \"-135\", \"-90\", \"-45\"]\n",
    "    legend_elements = [Line2D([0], [0], color=color_wheel[i], lw=2, label=polar_angles[i]) for i in range(N)]\n",
    "    # ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "    \n",
    "    # x_min, x_max = all_data[:, 0].min(), all_data[:, 0].max()\n",
    "    # y_min, y_max = all_data[:, 1].min(), all_data[:, 1].max()\n",
    "    # z_min, z_max = all_data[:, 2].min(), all_data[:, 2].max()\n",
    "\n",
    "    # ax1.set_title('3D Average Trajectories')\n",
    "    ax1.set_xlabel('PC1', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    # ax1.xaxis.pane.fill = False\n",
    "    ax1.set_ylabel('PC2', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax1.set_yticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    ax1.set_zlabel('PC3', fontsize=fnt, labelpad=-10, rotation=90)\n",
    "    ax1.set_zticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    # ax2.set_title('2D Projections of Average Trajectories')\n",
    "    # ax2.set_xlabel('PC1')\n",
    "    # ax2.set_ylabel('PC2')\n",
    "    ax1.grid(False)\n",
    "    # ax1.legend(handles=legend_elements, loc=\"upper right\", fontsize=fnt, frameon=False, bbox_to_anchor=(1.0, 0.5))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "N = 8 # 8\n",
    "T = 50\n",
    "fnt = 15\n",
    "\n",
    "# Assuming hs_pc_s data is available\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "top_rewarded_trials = get_top_rewarded_trials_new(dot_arr, r_arr, T, N)\n",
    "print('trt=',top_rewarded_trials.shape)\n",
    "plot_avg_trajectories(fig, hs_pc_s, top_rewarded_trials, N)\n",
    "# plot_trajectories(fig, hs_pc_s, top_rewarded_trials, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### colormap (bugged)\n",
    "# top T trajectories binned by dot vector (avg/all)\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def generate_bin_vectors(N):\n",
    "    angles = np.linspace(0, 2 * np.pi, N+1)[:-1]  # Omitting the last angle to avoid overlap with 0\n",
    "    return np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "\n",
    "# def has_duplicates(array_2d):\n",
    "#     flat_list = array_2d.flatten().tolist()\n",
    "#     unique_set = set(flat_list)\n",
    "#     return len(unique_set) < len(flat_list)\n",
    "\n",
    "def generate_polar_angles(N):\n",
    "    step = 360 / N\n",
    "    angles = [(i * step) % 360 for i in range(N)]\n",
    "    return [f\"{int(angle)}\" if angle <= 180 else f\"{int(angle - 360)}\" for angle in angles]\n",
    "\n",
    "def compute_dot_direction(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_dir = np.mean(dot_diffs, axis=1)\n",
    "    norm_dot_dir = np.linalg.norm(mean_dot_dir, axis=1)\n",
    "    mean_dot_dir_normalized = mean_dot_dir / norm_dot_dir[:, np.newaxis]\n",
    "    return mean_dot_dir_normalized\n",
    "\n",
    "def get_top_rewarded_trials_new(dot_arr, r_arr, T, N):\n",
    "    bins = generate_bin_vectors(N)\n",
    "    normalized_dot_dirs = compute_dot_direction(dot_arr)\n",
    "\n",
    "    # Assign each dot to a bin\n",
    "    bin_assignments = []\n",
    "    for dot in normalized_dot_dirs:\n",
    "        cosine_similarity = dot @ np.array(bins).T\n",
    "        assigned_bin = np.argmax(cosine_similarity)\n",
    "        bin_assignments.append(assigned_bin)\n",
    "\n",
    "    # Organize trials by their assigned bin and then sort them by total reward\n",
    "    total_rewards = np.sum(r_arr, axis=1)\n",
    "    bin_lists = [[] for _ in range(N)]\n",
    "    for i, assigned_bin in enumerate(bin_assignments):\n",
    "        bin_lists[assigned_bin].append(i)\n",
    "    \n",
    "    # Sort trials in each bin by reward and select top T trials\n",
    "    top_trials = np.zeros((T, N), dtype=int)\n",
    "    for idx, trial_list in enumerate(bin_lists):\n",
    "        sorted_trials = sorted(trial_list, key=lambda x: total_rewards[x], reverse=True)\n",
    "        top_trials[:, idx] = sorted_trials[:T]\n",
    "\n",
    "    return top_trials\n",
    "\n",
    "def plot_trajectories(fig, data, top_trials, N):\n",
    "    # Define a color wheel\n",
    "    color_wheel = plt.cm.hsv(np.linspace(0, 1, N+1))\n",
    "    custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', color_wheel)\n",
    "\n",
    "    clrs=['#ADD8E6', '#1E90FF', '#0000CD', '#00008B']\n",
    "\n",
    "    custom_cmap = ListedColormap(clrs)\n",
    "\n",
    "    # Create a ScalarMappable object with the colormap\n",
    "    norm = plt.Normalize(vmin=0, vmax=len(clrs)-1)\n",
    "    sm = ScalarMappable(norm=norm, cmap=custom_cmap)\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    for bin_idx, trials in enumerate(top_trials.T): # Transpose to iterate over trials per bin\n",
    "        for trial_idx in trials:\n",
    "            traj = data[trial_idx, :, :3]\n",
    "            segments_3d = [traj[i:i+2] for i in range(traj.shape[0]-1)]\n",
    "            segments_2d = [s[:, :2] for s in segments_3d]\n",
    "            \n",
    "            lc_3d = Line3DCollection(segments_3d, colors=color_wheel[bin_idx])\n",
    "            lc_2d = LineCollection(segments_2d, colors=color_wheel[bin_idx])\n",
    "            \n",
    "            # ax1.add_collection(lc_3d)\n",
    "            ax2.add_collection(lc_2d)\n",
    "    \n",
    "    # Set limits based on data\n",
    "    all_data = data[:, :, :3].reshape(-1, 3)\n",
    "    ax1.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax1.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax1.set_zlim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "    # ax2.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    # ax2.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "\n",
    "    polar_angles = generate_polar_angles(N) # [\"0\", \"45\", \"90\", \"135\", \"180\", \"-135\", \"-90\", \"-45\"]\n",
    "    # legend_elements = [Line2D([0], [0], color=color_wheel[i], lw=2, label=polar_angles[i]) for i in range(N)]\n",
    "    # ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "    \n",
    "    ax1.set_title('3D Trajectories')\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    # ax1.set_zlabel('PC3')\n",
    "    ax2.set_title('2D Projections of Trajectories')\n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_avg_trajectories(fig, data, top_trials, N):\n",
    "    # Define a color wheel\n",
    "    color_wheel = plt.cm.hsv(np.linspace(0, 1, N+1))\n",
    "    hsv_colors = plt.cm.hsv(np.linspace(0, 1, N))\n",
    "    custom_cmap = mpl.colors.ListedColormap(hsv_colors)\n",
    "    bounds = np.linspace(0, N, N + 1)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, custom_cmap.N)\n",
    "    # custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', color_wheel)\n",
    "\n",
    "    clrs=['#ADD8E6', '#1E90FF', '#0000CD', '#00008B']\n",
    "\n",
    "    custom_cmap = ListedColormap(clrs)\n",
    "\n",
    "    # Create a ScalarMappable object with the colormap\n",
    "    norm = plt.Normalize(vmin=0, vmax=len(clrs)-1)\n",
    "    sm = ScalarMappable(norm=norm, cmap=custom_cmap)\n",
    "    \n",
    "    # ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    for bin_idx, trials in enumerate(top_trials.T): \n",
    "        # Calculate the average trajectory for the trials in this bin\n",
    "        avg_traj = np.mean(data[trials], axis=0)\n",
    "        \n",
    "        segments_3d = [avg_traj[i:i+2] for i in range(avg_traj.shape[0]-1)]\n",
    "        segments_2d = [s[:, :2] for s in segments_3d]\n",
    "        \n",
    "        lc_3d = Line3DCollection(segments_3d, colors=color_wheel[bin_idx])\n",
    "        lc_2d = LineCollection(segments_2d, colors=color_wheel[bin_idx])\n",
    "        \n",
    "        # ax1.add_collection(lc_3d)\n",
    "        # ax2.add_collection(lc_2d)\n",
    "    \n",
    "    top_data_indices = top_trials.flatten()  # Assuming top_trials is a 2D array of indices\n",
    "    top_data = data[top_data_indices, :, :3].reshape(-1, 3)\n",
    "\n",
    "    # Define padding as a percentage of the data range\n",
    "    padding_factor = -0.0  # 5% padding\n",
    "\n",
    "    # Calculate the limits based on the top_data with padding\n",
    "    x_padding = (top_data[:, 0].max() - top_data[:, 0].min()) * padding_factor\n",
    "    y_padding = (top_data[:, 1].max() - top_data[:, 1].min()) * padding_factor\n",
    "    z_padding = (top_data[:, 2].max() - top_data[:, 2].min()) * padding_factor\n",
    "\n",
    "    x_range = (top_data[:, 0].min() - x_padding, top_data[:, 0].max() + x_padding)\n",
    "    y_range = (top_data[:, 1].min() - y_padding, top_data[:, 1].max() + y_padding)\n",
    "    z_range = (top_data[:, 2].min() - z_padding, top_data[:, 2].max() + z_padding)\n",
    "\n",
    "    # Set the limits for each axis based on the top_data with padding\n",
    "    ax2.set_xlim(x_range)\n",
    "    ax2.set_ylim(y_range)\n",
    "    # ax2.set_zlim(z_range)\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap=custom_cmap, norm=plt.Normalize(vmin=0, vmax=N))\n",
    "    sm.set_array([])  # You can set an array of values here if needed for scaling\n",
    "    cbar = plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=custom_cmap), cax=ax2, orientation='horizontal', ticks=[], aspect=10)\n",
    "    cbar.outline.set_edgecolor('none')  # Remove the colorbar border\n",
    "    cbar.ax.set_xticks([])  # Removes the ticks on the x-axis (for horizontal colorbar)\n",
    "    cbar.ax.set_yticks([])\n",
    "\n",
    "    # Manually adjust the size and position of the colorbar\n",
    "    cbar_width = ax2.get_position().width\n",
    "    cbar_height = 0.025  # Set the height of the colorbar\n",
    "    cbar_pos_x = ax2.get_position().x0\n",
    "    cbar_pos_y = ax2.get_position().y1 # + 0.05  # Adjust vertical position\n",
    "    cbar.ax.set_position([cbar_pos_x - 0.08, cbar_pos_y - .07, cbar_width, cbar_height])\n",
    "\n",
    "    # Remove tick marks from the colorbar\n",
    "    cbar.ax.xaxis.set_ticks_position('top')\n",
    "    cbar.ax.xaxis.set_tick_params(size=0)\n",
    "\n",
    "    # Add the 'time' label to the right\n",
    "    cbar.ax.text(1.1, 0.5, 'speed', verticalalignment='center', transform=cbar.ax.transAxes, fontsize=20)\n",
    "\n",
    "    # ax1.set_box_aspect([1,1,1]) \n",
    "    # ax2.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    # ax2.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    \n",
    "    # Create a legend with polar angles corresponding to the bins\n",
    "    polar_angles = generate_polar_angles(N) # [\"0\", \"45\", \"90\", \"135\", \"180\", \"-135\", \"-90\", \"-45\"]\n",
    "    legend_elements = [Line2D([0], [0], color=color_wheel[i], lw=2, label=polar_angles[i]) for i in range(N)]\n",
    "    # ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "    \n",
    "    # x_min, x_max = all_data[:, 0].min(), all_data[:, 0].max()\n",
    "    # y_min, y_max = all_data[:, 1].min(), all_data[:, 1].max()\n",
    "    # z_min, z_max = all_data[:, 2].min(), all_data[:, 2].max()\n",
    "\n",
    "    # ax1.set_title('3D Average Trajectories')\n",
    "    # ax2.set_xlabel('PC1', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax2.set_xticks([])\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['left'].set_linewidth(4)\n",
    "    ax2.spines['bottom'].set_linewidth(4)\n",
    "    # ax1.xaxis.pane.fill = False\n",
    "    # ax2.set_ylabel('PC2', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax2.set_yticks([])\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['left'].set_linewidth(4)\n",
    "    ax2.spines['bottom'].set_linewidth(4)\n",
    "    # ax2.set_zlabel('PC3', fontsize=fnt, labelpad=-10, rotation=90)\n",
    "    # ax2.set_zticks([])\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['left'].set_linewidth(4)\n",
    "    ax2.spines['bottom'].set_linewidth(4)\n",
    "    # ax2.set_title('2D Projections of Average Trajectories')\n",
    "    # ax2.set_xlabel('PC1')\n",
    "    # ax2.set_ylabel('PC2')\n",
    "    ax2.grid(False)\n",
    "    # ax1.legend(handles=legend_elements, loc=\"upper right\", fontsize=fnt, frameon=False, bbox_to_anchor=(1.0, 0.5))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "N = 8 # 8\n",
    "T = 60\n",
    "fnt = 16\n",
    "\n",
    "# Assuming hs_pc_s data is available\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "top_rewarded_trials = get_top_rewarded_trials_new(dot_arr, r_arr, T, N)\n",
    "print('trt=',top_rewarded_trials.shape)\n",
    "plot_avg_trajectories(fig, hs_pc_s, top_rewarded_trials, N)\n",
    "# plot_trajectories(fig, hs_pc_s, top_rewarded_trials, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend misc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# Assuming you have an array of angles and corresponding colors\n",
    "angles = np.linspace(0, 360, 360)  # Example angles from 0 to 360 degrees\n",
    "colors = plt.cm.hsv(angles / 360.)  # Corresponding colors using the HSV colormap\n",
    "\n",
    "# Create a figure and a subplot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create a LineCollection with a colormap\n",
    "cmap = ListedColormap(colors)\n",
    "norm = BoundaryNorm(angles, cmap.N)\n",
    "points = np.array([np.linspace(0, 1, 100), np.linspace(0, 0, 100)]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linewidth=2)\n",
    "lc.set_array(angles)\n",
    "\n",
    "# Add LineCollection to the axes for the legend\n",
    "ax.add_collection(lc)\n",
    "\n",
    "# Create a custom legend entry\n",
    "from matplotlib.legend_handler import HandlerLineCollection\n",
    "class HandlerColormap(HandlerLineCollection):\n",
    "    def __init__(self, cmap, num_stripes=10, **kw):\n",
    "        HandlerLineCollection.__init__(self, **kw)\n",
    "        self.cmap = cmap\n",
    "        self.num_stripes = num_stripes\n",
    "    def create_artists(self, legend, orig_handle, xdescent, ydescent, width, height, fontsize, trans):\n",
    "        stripes = [self.cmap(i * 1.0 / self.num_stripes) for i in range(self.num_stripes)]\n",
    "        lc = LineCollection([[(xdescent, ydescent + height / 2), (xdescent + width, ydescent + height / 2)]], \n",
    "                            colors=stripes, linewidth=orig_handle.get_linewidth())\n",
    "        return [lc]\n",
    "\n",
    "# Add legend to the plot\n",
    "plt.legend([lc], ['Angle'], handler_map={lc: HandlerColormap(cmap)}, handlelength=2)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories binned by dot vector - random (avg/all)\n",
    "# \n",
    "\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def generate_bin_vectors(N):\n",
    "    angles = np.linspace(0, 2 * np.pi, N+1)[:-1]  # Omitting the last angle to avoid overlap with 0\n",
    "    return np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "\n",
    "def generate_polar_angles(N):\n",
    "    step = 360 / N\n",
    "    angles = [(i * step) % 360 for i in range(N)]\n",
    "    return [f\"{int(angle)}\" if angle <= 180 else f\"{int(angle - 360)}\" for angle in angles]\n",
    "\n",
    "def compute_dot_direction(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_dir = np.mean(dot_diffs, axis=1)\n",
    "    norm_dot_dir = np.linalg.norm(mean_dot_dir, axis=1)\n",
    "    mean_dot_dir_normalized = mean_dot_dir / norm_dot_dir[:, np.newaxis]\n",
    "    return mean_dot_dir_normalized\n",
    "\n",
    "def get_binned_trials(dot_arr, N):\n",
    "    bins = generate_bin_vectors(N)\n",
    "    normalized_dot_dirs = compute_dot_direction(dot_arr)\n",
    "\n",
    "    # Assign each dot to a bin\n",
    "    bin_assignments = []\n",
    "    for dot in normalized_dot_dirs:\n",
    "        cosine_similarity = dot @ np.array(bins).T\n",
    "        assigned_bin = np.argmax(cosine_similarity)\n",
    "        bin_assignments.append(assigned_bin)\n",
    "\n",
    "    # Organize trials by their assigned bin\n",
    "    bin_lists = [[] for _ in range(N)]\n",
    "    for i, assigned_bin in enumerate(bin_assignments):\n",
    "        bin_lists[assigned_bin].append(i)\n",
    "\n",
    "    # Find the bin with the maximum number of trials\n",
    "    max_bin_length = max(len(trial_list) for trial_list in bin_lists)\n",
    "    \n",
    "    # Preallocate an array filled with nans\n",
    "    binned_trials = np.full((max_bin_length, N), np.nan)\n",
    "    \n",
    "    # Fill the array with trial indices for each bin\n",
    "    for idx, trial_list in enumerate(bin_lists):\n",
    "        binned_trials[:len(trial_list), idx] = trial_list\n",
    "\n",
    "    return binned_trials\n",
    "\n",
    "def plot_trajectories(fig, data, top_trials, N, T):\n",
    "    # define a color wheel\n",
    "    color_wheel = plt.cm.hsv(np.linspace(0, 1, N+1))\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    for bin_idx, trials in enumerate(top_trials.T): # Transpose to iterate over trials per bin\n",
    "        valid_trials = trials[~np.isnan(trials)].astype(int)  # Remove nan values and convert to int\n",
    "        \n",
    "        # select T random trials from valid trials\n",
    "        selected_trials = np.random.choice(valid_trials, min(T, len(valid_trials)), replace=False)\n",
    "        \n",
    "        for trial_idx in selected_trials:\n",
    "            traj = data[trial_idx, :, :3]\n",
    "            segments_3d = [traj[i:i+2] for i in range(traj.shape[0]-1)]\n",
    "            segments_2d = [s[:, 1:3] for s in segments_3d]\n",
    "            \n",
    "            lc_3d = Line3DCollection(segments_3d, colors=color_wheel[bin_idx])\n",
    "            lc_2d = LineCollection(segments_2d, colors=color_wheel[bin_idx])\n",
    "            \n",
    "            ax1.add_collection(lc_3d)\n",
    "            ax2.add_collection(lc_2d)\n",
    "    \n",
    "    # Set limits based on data\n",
    "    all_data = data[:, :, :3].reshape(-1, 3)\n",
    "    ax1.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax1.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax1.set_zlim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "    ax2.set_xlim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax2.set_ylim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "\n",
    "    polar_angles = generate_polar_angles(N) # [\"0\", \"45\", \"90\", \"135\", \"180\", \"-135\", \"-90\", \"-45\"]\n",
    "    legend_elements = [Line2D([0], [0], color=color_wheel[i], lw=2, label=polar_angles[i]) for i in range(N)]\n",
    "    ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "    \n",
    "    ax1.set_title('3D Trajectories')\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax2.set_title('2D Projections of Trajectories')\n",
    "    ax2.set_xlabel('PC2')\n",
    "    ax2.set_ylabel('PC3')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_avg_trajectories(fig, data, top_trials, N):\n",
    "    # define a color wheel\n",
    "    color_wheel = plt.cm.hsv(np.linspace(0, 1, N+1))\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    # for bin_idx, trials in enumerate(top_trials.T):\n",
    "    print('top_trials=',top_trials.shape)\n",
    "    k = 6\n",
    "    bin_idx = k\n",
    "    trials = top_trials.T[k]\n",
    "    valid_trials = trials[~np.isnan(trials)].astype(int)  # Remove nan values and convert to int\n",
    "    # calculate the average trajectory for the trials in this bin\n",
    "    avg_traj = np.mean(data[valid_trials], axis=0)\n",
    "    \n",
    "    segments_3d = [avg_traj[i:i+2] for i in range(avg_traj.shape[0]-1)]\n",
    "    segments_2d = [s[:, 1:3] for s in segments_3d]\n",
    "    \n",
    "    lc_3d = Line3DCollection(segments_3d, colors=color_wheel[bin_idx])\n",
    "    lc_2d = LineCollection(segments_2d, colors=color_wheel[bin_idx])\n",
    "    \n",
    "    ax1.add_collection(lc_3d)\n",
    "    ax2.add_collection(lc_2d)\n",
    "    \n",
    "    all_data = data[:, :, :3].reshape(-1, 3)\n",
    "    ax1.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax1.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax1.set_zlim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "    # ax2.set_xlim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    # ax2.set_ylim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "\n",
    "    top_data_indices = trials.flatten()  # Assuming top_trials is a 2D array of indices\n",
    "    top_data = data[top_data_indices.astype(int), :, :3].reshape(-1, 3)\n",
    "\n",
    "    # Define padding as a percentage of the data range\n",
    "    padding_factor = -0.3  # 5% padding\n",
    "\n",
    "    # Calculate the limits based on the top_data with padding\n",
    "    x_padding = (top_data[:, 0].max() - top_data[:, 0].min()) * padding_factor\n",
    "    y_padding = (top_data[:, 1].max() - top_data[:, 1].min()) * padding_factor\n",
    "    z_padding = (top_data[:, 2].max() - top_data[:, 2].min()) * padding_factor\n",
    "\n",
    "    x_range = (top_data[:, 0].min() - x_padding, top_data[:, 0].max() + x_padding)\n",
    "    y_range = (top_data[:, 1].min() - y_padding, top_data[:, 1].max() + y_padding)\n",
    "    z_range = (top_data[:, 2].min() - z_padding, top_data[:, 2].max() + z_padding)\n",
    "\n",
    "    # Set the limits for each axis based on the top_data with padding\n",
    "    ax1.set_xlim(x_range)\n",
    "    ax1.set_ylim(y_range)\n",
    "    ax1.set_zlim(z_range)\n",
    "    \n",
    "    polar_angles = generate_polar_angles(N) # [\"0\", \"45\", \"90\", \"135\", \"180\", \"-135\", \"-90\", \"-45\"]\n",
    "    legend_elements = [Line2D([0], [0], color=color_wheel[i], lw=2, label=polar_angles[i]) for i in range(N)]\n",
    "    # ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "    \n",
    "    ax1.set_xlabel('PC1', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    # ax1.xaxis.pane.fill = False\n",
    "    ax1.set_ylabel('PC2', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax1.set_yticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    ax1.set_zlabel('PC3', fontsize=fnt, labelpad=-5, rotation=90)\n",
    "    ax1.set_zticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    ax1.grid(False)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "N = 10\n",
    "# T = 110\n",
    "fnt = 18\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# top_rewarded_trials = get_random_trials(dot_arr, T, N)\n",
    "top_rewarded_trials = get_binned_trials(dot_arr, N)\n",
    "print('trt=',top_rewarded_trials.shape)\n",
    "plot_avg_trajectories(fig, hs_pc_s, top_rewarded_trials, N)\n",
    "# plot_avg_trajectories(fig, hv_pc_s, top_rewarded_trials, N)\n",
    "\n",
    "# plot_trajectories(fig, hs_pc_s, top_rewarded_trials, N, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### single trial\n",
    "### new vers\n",
    "\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def generate_bin_vectors(N):\n",
    "    angles = np.linspace(0, 2 * np.pi, N+1)[:-1]\n",
    "    return np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "\n",
    "def generate_polar_angles(N):\n",
    "    step = 360 / N\n",
    "    angles = [(i * step) % 360 for i in range(N)]\n",
    "    return [f\"{int(angle)}\" if angle <= 180 else f\"{int(angle - 360)}\" for angle in angles]\n",
    "\n",
    "def compute_dot_direction(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_dir = np.mean(dot_diffs, axis=1)\n",
    "    norm_dot_dir = np.linalg.norm(mean_dot_dir, axis=1)\n",
    "    mean_dot_dir_normalized = mean_dot_dir / norm_dot_dir[:, np.newaxis]\n",
    "    return mean_dot_dir_normalized\n",
    "\n",
    "def get_binned_trials(dot_arr, N):\n",
    "    bins = generate_bin_vectors(N)\n",
    "    normalized_dot_dirs = compute_dot_direction(dot_arr)\n",
    "    bin_assignments = []\n",
    "    for dot in normalized_dot_dirs:\n",
    "        cosine_similarity = dot @ bins.T\n",
    "        assigned_bin = np.argmax(cosine_similarity)\n",
    "        bin_assignments.append(assigned_bin)\n",
    "    bin_lists = [[] for _ in range(N)]\n",
    "    for i, assigned_bin in enumerate(bin_assignments):\n",
    "        bin_lists[assigned_bin].append(i)\n",
    "    max_bin_length = max(len(trial_list) for trial_list in bin_lists)\n",
    "    binned_trials = np.full((max_bin_length, N), np.nan)\n",
    "    for idx, trial_list in enumerate(bin_lists):\n",
    "        binned_trials[:len(trial_list), idx] = trial_list\n",
    "    return binned_trials\n",
    "\n",
    "def plot_avg_trajectories(fig, data, top_trials, N):\n",
    "    color_map = plt.cm.get_cmap('seismic')\n",
    "    ax1 = fig.add_subplot(111, projection='3d')\n",
    "    # ax2 = fig.add_subplot(122)\n",
    "    trial_idx = int(top_trials[0, 0])  # Example: Select the first trial for plotting\n",
    "\n",
    "    k = 5\n",
    "    bin_idx = k\n",
    "    trials = top_trials.T[k]\n",
    "    valid_trials = trials[~np.isnan(trials)].astype(int)  # Remove nan values and convert to int\n",
    "    # calculate the average trajectory for the trials in this bin\n",
    "    avg_traj = np.mean(data[valid_trials], axis=0)\n",
    "\n",
    "    traj = data[trial_idx, :, :3]\n",
    "    num_points = traj.shape[0]\n",
    "    angle_values = np.linspace(0, 1, num_points)\n",
    "\n",
    "    segments = [avg_traj[i:i+2] for i in range(num_points - 1)]\n",
    "    lc = Line3DCollection(segments, cmap=color_map, norm=plt.Normalize(0, 1))\n",
    "    lc.set_array(angle_values)\n",
    "    ax1.add_collection3d(lc)\n",
    "\n",
    "    top_data_indices = trials.flatten()  # Assuming top_trials is a 2D array of indices\n",
    "    top_data = data[top_data_indices.astype(int), :, :3].reshape(-1, 3)\n",
    "\n",
    "    # Define padding as a percentage of the data range\n",
    "    padding_factor = -0.29  # 5% padding\n",
    "\n",
    "    # Calculate the limits based on the top_data with padding\n",
    "    x_padding = (top_data[:, 0].max() - top_data[:, 0].min()) * padding_factor\n",
    "    y_padding = (top_data[:, 1].max() - top_data[:, 1].min()) * padding_factor\n",
    "    z_padding = (top_data[:, 2].max() - top_data[:, 2].min()) * padding_factor\n",
    "\n",
    "    x_range = (top_data[:, 0].min() - x_padding, top_data[:, 0].max() + x_padding)\n",
    "    y_range = (top_data[:, 1].min() - y_padding, top_data[:, 1].max() + y_padding)\n",
    "    z_range = (top_data[:, 2].min() - z_padding, top_data[:, 2].max() + z_padding)\n",
    "\n",
    "    # Set the limits for each axis based on the top_data with padding\n",
    "    ax1.set_xlim(x_range)\n",
    "    ax1.set_ylim(y_range)\n",
    "    ax1.set_zlim(z_range)\n",
    "    cbar = fig.colorbar(lc, ax=ax1, orientation='horizontal', pad=0.1, ticks=[], aspect=4)\n",
    "    cbar.outline.set_edgecolor('none')\n",
    "\n",
    "    # Manually adjust the size and position of the colorbar\n",
    "    cbar_width = ax1.get_position().width\n",
    "    cbar_height = 0.025  # Set the height of the colorbar\n",
    "    cbar_pos_x = ax1.get_position().x0\n",
    "    cbar_pos_y = ax1.get_position().y1 # + 0.05  # Adjust vertical position\n",
    "    cbar.ax.set_position([cbar_pos_x - .05, cbar_pos_y - .04, cbar_width, cbar_height])\n",
    "\n",
    "    # Remove tick marks from the colorbar\n",
    "    cbar.ax.xaxis.set_ticks_position('top')\n",
    "    cbar.ax.xaxis.set_tick_params(size=0)\n",
    "\n",
    "    # Add the 'time' label to the right\n",
    "    cbar.ax.text(1.2, 0.5, 'time', verticalalignment='center', transform=cbar.ax.transAxes, fontsize=14)\n",
    "\n",
    "    ax1.set_xlabel('PC1', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    # ax1.xaxis.pane.fill = False\n",
    "    ax1.set_ylabel('PC2', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax1.set_yticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    ax1.set_zlabel('PC3', fontsize=fnt, labelpad=-10, rotation=90)\n",
    "    ax1.set_zticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    ax1.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Replace with your actual data\n",
    "# dot_arr = ...\n",
    "# hs_pc_s = ...\n",
    "\n",
    "N = 10\n",
    "fnt = 18\n",
    "\n",
    "fig = plt.figure(figsize=(4, 6))\n",
    "top_rewarded_trials = get_binned_trials(dot_arr, N)\n",
    "plot_avg_trajectories(fig, hs_pc_s, top_rewarded_trials, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dot speed\n",
    "### traj binned by dot speed (avg)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "# clrs=['#ADD8E6','#1E90FF','#00008B']\n",
    "clrs=['#ADD8E6', '#1E90FF', '#0000CD', '#00008B']\n",
    "\n",
    "# cmap = ListedColormap(clrs)\n",
    "\n",
    "# # Create a ScalarMappable object with the colormap\n",
    "# norm = plt.Normalize(vmin=0, vmax=len(clrs)-1)\n",
    "# sm = ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "def generate_bin_vectors(N):\n",
    "    angles = np.linspace(0, 2 * np.pi, N+1)[:-1]\n",
    "    return np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "\n",
    "def compute_dot_speed(dot_arr):\n",
    "    speeds_per_timestep = np.linalg.norm(dot_arr, axis=2)\n",
    "    avg_speeds_per_trial = np.mean(speeds_per_timestep, axis=1)\n",
    "    return avg_speeds_per_trial\n",
    "\n",
    "def bin_by_dot_speed(dot_speeds, sbins):\n",
    "    bin_edges = np.linspace(dot_speeds.min(), dot_speeds.max(), sbins + 1)\n",
    "    bin_indices = np.digitize(dot_speeds, bin_edges) - 1\n",
    "    return bin_indices\n",
    "\n",
    "def get_avg_trajectories(data, bin_indices, sbins):\n",
    "    avg_trajectories = []\n",
    "    for i in range(sbins):\n",
    "        avg_trajectories.append(np.mean(data[bin_indices == i], axis=0))\n",
    "    return avg_trajectories\n",
    "\n",
    "def plot_avg_trajectories_by_speed(fig, data, dot_speeds, sbins):\n",
    "    color_wheel = plt.cm.jet(np.linspace(0, 1, sbins))\n",
    "    \n",
    "    bin_indices = bin_by_dot_speed(dot_speeds, sbins)\n",
    "\n",
    "    avg_trajectories = get_avg_trajectories(data, bin_indices, sbins)\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    # ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    for bin_idx, avg_traj in enumerate(avg_trajectories): \n",
    "        print('bin_idx=',bin_idx)\n",
    "        segments_3d = [avg_traj[i:i+2] for i in range(avg_traj.shape[0]-1)]\n",
    "        segments_2d = [s[:, :2] for s in segments_3d]\n",
    "        \n",
    "        lc_3d = Line3DCollection(segments_3d, colors=clrs[bin_idx])\n",
    "        # lc_2d = LineCollection(segments_2d, colors=clrs[bin_idx])\n",
    "        \n",
    "        ax1.add_collection(lc_3d)\n",
    "        # ax2.add_collection(lc_2d)\n",
    "    \n",
    "    all_data = data.reshape(-1, 3)\n",
    "    ax1.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax1.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax1.set_zlim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "    # ax2.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    # ax2.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    \n",
    "    bin_labels = [\"speed \" + str(i+1) for i in range(sbins)]\n",
    "    legend_elements = [Line2D([0], [0], color=clrs[i], lw=2, label=bin_labels[i]) for i in range(sbins)]\n",
    "    \n",
    "    # ax1.set_title('3D Average Trajectories by Dot Speed')\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    # ax2.set_title('2D Projections of Average Trajectories by Dot Speed')\n",
    "    # ax2.set_xlabel('PC1')\n",
    "    # ax2.set_ylabel('PC2')\n",
    "\n",
    "    ax1.set_xlabel('PC1', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    # ax1.xaxis.pane.fill = False\n",
    "    ax1.set_ylabel('PC2', fontsize=fnt, labelpad=-0, rotation=0)\n",
    "    ax1.set_yticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    ax1.set_zlabel('PC3', fontsize=fnt, labelpad=-10, rotation=90)\n",
    "    ax1.set_zticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    ax1.grid(False)\n",
    "\n",
    "    ax1.legend(handles=legend_elements, loc=\"upper center\", ncol=4, fontsize=14, frameon=False, handlelength=1.5, columnspacing=0.5, bbox_to_anchor=(0.53, 1.05))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "sbins = 4\n",
    "# Assuming hs_pc_s data and dot_arr are available\n",
    "fnt = 18\n",
    "\n",
    "dot_speeds = compute_dot_speed(dot_arr)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plot_avg_trajectories_by_speed(fig, hs_pc_s, dot_speeds, sbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed binned, single direction (avg)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def compute_dot_direction(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_dir = np.mean(dot_diffs, axis=1)\n",
    "    norm_dot_dir = np.linalg.norm(mean_dot_dir, axis=1)\n",
    "    mean_dot_dir_normalized = mean_dot_dir / norm_dot_dir[:,np.newaxis]\n",
    "    return mean_dot_dir_normalized\n",
    "\n",
    "def bin_by_dot_direction(dot_directions, bins):\n",
    "    cosine_similarity = dot_directions @ np.array(bins).T\n",
    "    bin_indices = np.argmax(cosine_similarity, axis=1)\n",
    "    return bin_indices\n",
    "\n",
    "def compute_dot_speed(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    speeds = np.linalg.norm(dot_diffs, axis=2)\n",
    "    avg_speeds = np.mean(speeds, axis=1)\n",
    "    return avg_speeds\n",
    "\n",
    "def bin_by_speed(dot_speeds, sbins):\n",
    "    bin_edges = np.linspace(dot_speeds.min(), dot_speeds.max(), sbins + 1)\n",
    "    bin_indices = np.digitize(dot_speeds, bin_edges) - 1\n",
    "    return bin_indices\n",
    "\n",
    "def plot_avg_trajectories_by_direction_and_speed(fig, data, dot_directions, bins, dbin, dot_speeds, sbins):\n",
    "    # Define a color wheel\n",
    "    color_wheel = plt.cm.jet(np.linspace(0, 1, sbins))\n",
    "    \n",
    "    # Compute the binned indices for directions and speeds\n",
    "    direction_indices = bin_by_dot_direction(dot_directions, bins)\n",
    "    speed_indices = bin_by_speed(dot_speeds, sbins)\n",
    "\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    # For the chosen direction bin, further bin by speed and plot averaged trajectories\n",
    "    for speed_bin_idx in range(sbins):\n",
    "        selected_indices = np.logical_and(direction_indices == dbin, speed_indices == speed_bin_idx)\n",
    "        avg_traj = np.mean(data[selected_indices], axis=0)\n",
    "        \n",
    "        segments_3d = [avg_traj[i:i+2] for i in range(avg_traj.shape[0]-1)]\n",
    "        segments_2d = [s[:, :2] for s in segments_3d]\n",
    "        \n",
    "        lc_3d = Line3DCollection(segments_3d, colors=color_wheel[speed_bin_idx])\n",
    "        lc_2d = LineCollection(segments_2d, colors=color_wheel[speed_bin_idx])\n",
    "        \n",
    "        ax1.add_collection(lc_3d)\n",
    "        ax2.add_collection(lc_2d)\n",
    "    \n",
    "    # Set limits based on data\n",
    "    all_data = data.reshape(-1, 3)\n",
    "    ax1.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax1.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax1.set_zlim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "    ax2.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax2.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    \n",
    "    # Create a legend based on speed bins\n",
    "    bin_labels = [\"Speed Bin \" + str(i+1) for i in range(sbins)]\n",
    "    legend_elements = [Line2D([0], [0], color=color_wheel[i], lw=2, label=bin_labels[i]) for i in range(sbins)]\n",
    "    \n",
    "    ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "    \n",
    "    ax1.set_title(f'3D Average Trajectories by Speed (Direction Bin {dbin})')\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax2.set_title(f'2D Projections of Average Trajectories by Speed (Direction Bin {dbin})')\n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "N = 8\n",
    "bins = generate_bin_vectors(N)\n",
    "dot_directions = compute_dot_direction(dot_arr)\n",
    "dot_speeds = compute_dot_speed(dot_arr)\n",
    "dbin = 4\n",
    "sbins = 3\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "# plot_avg_trajectories_by_direction_and_speed(fig, hs_pc_s, dot_directions, bins, dbin, dot_speeds, sbins)\n",
    "plot_avg_trajectories_by_direction_and_speed(fig, hv_pc_s, dot_directions, bins, dbin, dot_speeds, sbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed, direction binned all (avg)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def generate_bin_vectors(N):\n",
    "    angles = np.linspace(0, 2 * np.pi, N+1)[:-1]\n",
    "    return np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "\n",
    "def compute_dot_direction(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_dir = np.mean(dot_diffs, axis=1)\n",
    "    norm_dot_dir = np.linalg.norm(mean_dot_dir, axis=1)\n",
    "    mean_dot_dir_normalized = mean_dot_dir / norm_dot_dir[:,np.newaxis]\n",
    "    return mean_dot_dir_normalized\n",
    "\n",
    "def bin_by_dot_direction(dot_directions, bins):\n",
    "    cosine_similarity = dot_directions @ np.array(bins).T\n",
    "    bin_indices = np.argmax(cosine_similarity, axis=1)\n",
    "    return bin_indices\n",
    "\n",
    "def compute_dot_speed(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    speeds = np.linalg.norm(dot_diffs, axis=2)\n",
    "    avg_speeds = np.mean(speeds, axis=1)\n",
    "    return avg_speeds\n",
    "\n",
    "def bin_by_speed(dot_speeds, sbins):\n",
    "    bin_edges = np.linspace(dot_speeds.min(), dot_speeds.max(), sbins + 1)\n",
    "    bin_indices = np.digitize(dot_speeds, bin_edges) - 1\n",
    "    return bin_indices\n",
    "\n",
    "def plot_avg_trajectories_by_all_directions(fig, data, dot_directions, bins, dot_speeds, sbins):\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    direction_indices = bin_by_dot_direction(dot_directions, bins)\n",
    "    speed_indices = bin_by_speed(dot_speeds, sbins)\n",
    "\n",
    "    # hsv colormap based on direction\n",
    "    hue_values = np.linspace(0, 1, len(bins) + 1)[:-1]\n",
    "\n",
    "    for dbin, hue in zip(range(len(bins)), hue_values):\n",
    "        for speed_bin_idx in range(sbins):\n",
    "            selected_indices = np.logical_and(direction_indices == dbin, speed_indices == speed_bin_idx)\n",
    "            avg_traj = np.mean(data[selected_indices], axis=0)\n",
    "            \n",
    "            segments_3d = [avg_traj[i:i+2] for i in range(avg_traj.shape[0]-1)]\n",
    "            segments_2d = [s[:, :2] for s in segments_3d]\n",
    "            \n",
    "            # darker color for faster speed bins\n",
    "            color = hsv_to_rgb((hue, 1, 1 - speed_bin_idx / (sbins + 1)))\n",
    "            \n",
    "            lc_3d = Line3DCollection(segments_3d, colors=color)\n",
    "            lc_2d = LineCollection(segments_2d, colors=color)\n",
    "            \n",
    "            ax1.add_collection(lc_3d)\n",
    "            ax2.add_collection(lc_2d)\n",
    "            \n",
    "            # optional, plot grey dot at the 15th timestep\n",
    "            # ax1.scatter(avg_traj[14, 0], avg_traj[14, 1], avg_traj[14, 2], color='grey', s=30)\n",
    "            # ax2.plot(avg_traj[14, 0], avg_traj[14, 1], 'o', color='grey', markersize=4)\n",
    "\n",
    "    # Set limits\n",
    "    all_data = data.reshape(-1, 3)\n",
    "    ax1.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax1.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax1.set_zlim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "    ax2.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax2.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "\n",
    "    ax1.set_title('3D Average Trajectories by Direction and Speed')\n",
    "    ax2.set_title('2D Projections of Average Trajectories by Direction and Speed')\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2')\n",
    "\n",
    "    # Legend on the ax2 subplot\n",
    "    legend_elements = [Line2D([0], [0], color=hsv_to_rgb((hue, 1, 0.5)), lw=2, label=f\"Direction Bin {dbin}\") for dbin, hue in zip(range(len(bins)), hue_values)]\n",
    "    ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "N = 3\n",
    "bins = generate_bin_vectors(N)\n",
    "dot_directions = compute_dot_direction(dot_arr)\n",
    "dot_speeds = compute_dot_speed(dot_arr)\n",
    "sbins = 2\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))  \n",
    "# plot_avg_trajectories_by_all_directions(fig, hs_pc_s, dot_directions, bins, dot_speeds, sbins)\n",
    "plot_avg_trajectories_by_all_directions(fig, hv_pc_s, dot_directions, bins, dot_speeds, sbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed, direction binned (old/doesnt work)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from matplotlib.collections import LineCollection\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def generate_bin_vectors(N):\n",
    "    angles = np.linspace(0, 2 * np.pi, N+1)[:-1]\n",
    "    return np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "\n",
    "def compute_dot_direction(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_dir = np.mean(dot_diffs, axis=1)\n",
    "    norm_dot_dir = np.linalg.norm(mean_dot_dir, axis=1)\n",
    "    mean_dot_dir_normalized = mean_dot_dir / norm_dot_dir[:, np.newaxis]\n",
    "    return mean_dot_dir_normalized\n",
    "\n",
    "def compute_speed(dot_arr):\n",
    "    diff = np.diff(dot_arr, axis=1)\n",
    "    speed = np.linalg.norm(diff, axis=2)\n",
    "    avg_speed = np.mean(speed, axis=1)\n",
    "    return avg_speed\n",
    "\n",
    "def get_binned_indices(dot_arr, dbins, sbins):\n",
    "    bins = generate_bin_vectors(dbins)\n",
    "    dot_dirs = compute_dot_direction(dot_arr)\n",
    "    speeds = compute_speed(dot_arr)\n",
    "    speed_bins = np.linspace(np.min(speeds), np.max(speeds), sbins+1)\n",
    "    \n",
    "    bin_indices = {}\n",
    "    \n",
    "    for i, dot in enumerate(dot_dirs):\n",
    "        # Find dot vector bin\n",
    "        cosine_similarity = dot @ bins.T\n",
    "        d_bin = np.argmax(cosine_similarity)\n",
    "\n",
    "        # Find speed bin\n",
    "        s_bin = np.digitize(speeds[i], speed_bins) - 1\n",
    "\n",
    "        if (d_bin, s_bin) not in bin_indices:\n",
    "            bin_indices[(d_bin, s_bin)] = []\n",
    "        bin_indices[(d_bin, s_bin)].append(i)\n",
    "    \n",
    "    return bin_indices\n",
    "\n",
    "def plot_avg_trajectories_by_bins(fig, data, dot_arr, dbins, sbins):   \n",
    "    avg_dot_vectors = np.mean(np.linalg.norm(dot_arr, axis=-1), axis=-1)\n",
    "    d_bin_edges = np.linspace(avg_dot_vectors.min(), avg_dot_vectors.max(), dbins+1)\n",
    "\n",
    "    # Calculate speed for each trial as the mean speed over timesteps\n",
    "    speeds = np.linalg.norm(np.diff(data, axis=1), axis=2).mean(axis=1)\n",
    "\n",
    "    bin_indices = defaultdict(list)\n",
    "    s_bin_edges_list = []\n",
    "\n",
    "    for d in range(dbins):\n",
    "        d_mask = (dot_arr >= d_bin_edges[d]) & (dot_arr < d_bin_edges[d+1])\n",
    "        \n",
    "        if np.sum(d_mask) == 0:  # No data points in this dot bin\n",
    "            s_bin_edges_list.append(np.linspace(0, 1, sbins+1))  # Dummy values\n",
    "            continue\n",
    "        \n",
    "        relevant_speeds = speeds[d_mask]\n",
    "        s_bin_edges = np.linspace(relevant_speeds.min(), relevant_speeds.max(), sbins+1)\n",
    "        s_bin_edges_list.append(s_bin_edges)\n",
    "        \n",
    "        for s in range(sbins):\n",
    "            s_mask = (speeds >= s_bin_edges[s]) & (speeds < s_bin_edges[s+1])\n",
    "            combined_mask = d_mask & s_mask\n",
    "            bin_indices[(d, s)].extend(np.where(combined_mask)[0])\n",
    "            \n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    color_wheel = plt.cm.jet(np.linspace(0, 1, dbins*sbins))\n",
    "    \n",
    "    for idx, trials in enumerate(bin_indices.values()):\n",
    "        avg_traj = np.mean(data[trials], axis=0)\n",
    "        ax.plot(avg_traj[:, 0], avg_traj[:, 1], color=color_wheel[idx])\n",
    "        \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Averaged Trajectories Binned by Dot Vector and Speed')\n",
    "    plt.show()\n",
    "\n",
    "dbins = 8\n",
    "sbins = 2\n",
    "\n",
    "# Usage example (assuming you've loaded or generated 'dot_arr' and 'hs_pc_s'):\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plot_avg_trajectories_by_bins(fig, hs_pc_s, dot_arr, dbins, sbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed, direction binned (old/doesnt work)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def compute_dot_speed(dot_arr):\n",
    "    \"\"\"Compute the average dot speed across timesteps for each trial.\"\"\"\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_vel = np.mean(dot_diffs, axis=1)\n",
    "    mean_dot_speeds = np.linalg.norm(mean_dot_vel, axis=1)\n",
    "    return mean_dot_speeds\n",
    "\n",
    "def bin_by_direction(dot_directions, dbins):\n",
    "    bin_edges = np.linspace(-1, 1, dbins + 1)  # Assuming directions range from -1 to 1\n",
    "    bin_indices = np.digitize(dot_directions, bin_edges) - 1\n",
    "    return bin_indices\n",
    "\n",
    "def bin_by_speed(dot_speeds, sbins):\n",
    "    bin_edges = np.linspace(dot_speeds.min(), dot_speeds.max() + 1e-10, sbins + 1)\n",
    "    bin_indices = np.digitize(dot_speeds, bin_edges) - 1\n",
    "    return bin_indices\n",
    "\n",
    "def plot_avg_trajectories_by_direction_and_speed(fig, data, dot_directions_bins, dbins, dot_speeds, sbins):\n",
    "    # Define primary colors for the direction bins\n",
    "    primary_colors = plt.cm.tab10(np.linspace(0, 1, dbins))\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    legend_elements = []\n",
    "    \n",
    "    for dir_bin_idx in range(dbins):\n",
    "        # Get shades for speed bins within the primary color\n",
    "        shades = [primary_colors[dir_bin_idx] * (0.5 + 0.5 * (i+1)/sbins) for i in range(sbins)]\n",
    "        \n",
    "        # Create legend for current direction bin\n",
    "        for speed_bin_idx in range(sbins):\n",
    "            legend_elements.append(Line2D([0], [0], color=shades[speed_bin_idx], lw=2, \n",
    "                                        label=f\"Direction {dir_bin_idx + 1}, Speed Bin {speed_bin_idx + 1}\"))\n",
    "        \n",
    "        for speed_bin_idx in range(sbins):\n",
    "            selected_indices = np.logical_and(dot_directions_bins[:, 0] == dir_bin_idx, \n",
    "                                            bin_by_speed(dot_speeds, sbins) == speed_bin_idx)\n",
    "            \n",
    "            # Debugging print statement\n",
    "            print(f\"Direction {dir_bin_idx + 1}, Speed Bin {speed_bin_idx + 1}: {selected_indices.sum()} selected trials\")\n",
    "            \n",
    "            avg_traj = np.mean(data[selected_indices], axis=0)\n",
    "            \n",
    "            # Debugging print statement\n",
    "            if np.isnan(avg_traj).any():\n",
    "                print(f\"Direction {dir_bin_idx + 1}, Speed Bin {speed_bin_idx + 1} has NaN values in the averaged trajectory.\")\n",
    "            \n",
    "            segments_3d = [avg_traj[i:i+2] for i in range(avg_traj.shape[0]-1)]\n",
    "            segments_2d = [s[:, :2] for s in segments_3d]\n",
    "            \n",
    "            lc_3d = Line3DCollection(segments_3d, colors=shades[speed_bin_idx])\n",
    "            lc_2d = LineCollection(segments_2d, colors=shades[speed_bin_idx])\n",
    "            \n",
    "            ax1.add_collection(lc_3d)\n",
    "            ax2.add_collection(lc_2d)\n",
    "    \n",
    "    # Set limits based on data\n",
    "    all_data = data.reshape(-1, 3)\n",
    "    ax1.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax1.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    ax1.set_zlim(all_data[:, 2].min(), all_data[:, 2].max())\n",
    "    ax2.set_xlim(all_data[:, 0].min(), all_data[:, 0].max())\n",
    "    ax2.set_ylim(all_data[:, 1].min(), all_data[:, 1].max())\n",
    "    \n",
    "    ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "    \n",
    "    ax1.set_title('3D Average Trajectories by Direction and Speed')\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax2.set_title('2D Projections of Average Trajectories by Direction and Speed')\n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your bins and hs_pc_s data are available\n",
    "dbins = 8\n",
    "sbins = 2\n",
    "\n",
    "dot_directions = compute_dot_direction(dot_arr)\n",
    "dot_directions_bins = bin_by_direction(dot_directions, dbins)\n",
    "dot_speeds = compute_dot_speed(dot_arr)\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "plot_avg_trajectories_by_direction_and_speed(fig, hs_pc_s, dot_directions_bins, dbins, dot_speeds, sbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug (hist)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dot_arr is defined and normalized\n",
    "norms = np.linalg.norm(dot_arr, axis=2)\n",
    "normalized_dot_arr = dot_arr / norms[:, :, np.newaxis]\n",
    "\n",
    "# Compute the angles of these vectors in degrees\n",
    "angles = np.arctan2(normalized_dot_arr[:,:,1], normalized_dot_arr[:,:,0]) \n",
    "angles_deg = np.degrees(angles) % 360  # Convert to degrees and ensure the range is [0, 360)\n",
    "\n",
    "# Flatten the angles array for the histogram\n",
    "angles_flat = angles_deg.flatten()\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(angles_flat, bins=360, range=(0,360), color='blue', alpha=0.7)\n",
    "plt.title(\"Histogram of Directions of Normalized Dot Vectors\")\n",
    "plt.xlabel(\"Direction (degrees)\")\n",
    "plt.ylabel(\"Number of Dot Vectors\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug (scatter)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your bins are defined elsewhere\n",
    "bins = generate_bin_vectors(N)\n",
    "\n",
    "# Calculate the average dot velocity vectors\n",
    "dot_diffs = np.diff(dot_arr, axis=1)\n",
    "mean_dot_vel = np.mean(dot_diffs, axis=1)\n",
    "norm_dot_vel = np.linalg.norm(mean_dot_vel, axis=1)\n",
    "normalized_dot_vel = mean_dot_vel / norm_dot_vel[:, np.newaxis]\n",
    "\n",
    "# Assign each normalized dot velocity vector to a bin\n",
    "bin_assignments = []\n",
    "for dot in normalized_dot_vel:\n",
    "    cosine_similarity = dot @ np.array(bins).T\n",
    "    assigned_bin = np.argmax(cosine_similarity)\n",
    "    bin_assignments.append(assigned_bin)\n",
    "\n",
    "# Scatter plot each bin\n",
    "plt.figure(figsize=(10,10))\n",
    "for idx, bin_val in enumerate(bins):\n",
    "    selected_dots = normalized_dot_vel[np.array(bin_assignments) == idx] # normalized_dot_vel[np.array(bin_assignments) == idx]\n",
    "    print('idx=',idx,'selected_dots.shape=',selected_dots.shape)\n",
    "    plt.scatter(selected_dots[:, 0], selected_dots[:, 1], label=f'Bin {idx}')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Spread of normalized dot velocity vectors for each bin\")\n",
    "plt.xlabel(\"X-coordinate\")\n",
    "plt.ylabel(\"Y-coordinate\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print('bin_assignments=',bin_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug (scatter 2)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_bin_vectors(N):\n",
    "    angles = np.linspace(0, 2 * np.pi, N+1)[:-1]  # Omitting the last angle to avoid overlap with 0\n",
    "    return np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "\n",
    "def compute_dot_direction(dot_arr):\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_dir = np.mean(dot_diffs, axis=1)\n",
    "    norm_dot_dir = np.linalg.norm(mean_dot_dir, axis=1)\n",
    "    mean_dot_dir_normalized = mean_dot_dir / norm_dot_dir[:, np.newaxis]\n",
    "    return mean_dot_dir_normalized\n",
    "\n",
    "def get_top_rewarded_trials_new(dot_arr, r_arr, T, N):\n",
    "    bins = generate_bin_vectors(N)\n",
    "    normalized_dot_dirs = compute_dot_direction(dot_arr)\n",
    "\n",
    "    # Assign each dot to a bin\n",
    "    bin_assignments = []\n",
    "    for dot in normalized_dot_dirs:\n",
    "        cosine_similarity = dot @ np.array(bins).T\n",
    "        assigned_bin = np.argmax(cosine_similarity)\n",
    "        bin_assignments.append(assigned_bin)\n",
    "\n",
    "    # Organize trials by their assigned bin and then sort them by total reward\n",
    "    total_rewards = np.sum(r_arr, axis=1)\n",
    "    bin_lists = [[] for _ in range(N)]\n",
    "    for i, assigned_bin in enumerate(bin_assignments):\n",
    "        bin_lists[assigned_bin].append(i)\n",
    "    \n",
    "    # Sort trials in each bin by reward and select top T trials\n",
    "    top_trials = np.zeros((T, N), dtype=int)\n",
    "    for idx, trial_list in enumerate(bin_lists):\n",
    "        sorted_trials = sorted(trial_list, key=lambda x: total_rewards[x], reverse=True)\n",
    "        top_trials[:, idx] = sorted_trials[:T]\n",
    "\n",
    "    return top_trials\n",
    "\n",
    "def get_top_rewarded_trials_old(dot_arr, r_arr, T, N):\n",
    "    norms = np.linalg.norm(dot_arr, axis=2)\n",
    "    normalized_dot_arr = dot_arr / norms[:, :, np.newaxis]\n",
    "    bins = generate_bin_vectors(N)\n",
    "    \n",
    "    # Create an array to store the top K trials for each bin\n",
    "    top_trials = np.empty((T, N), dtype=int)\n",
    "\n",
    "    # Calculate the sum of rewards for each trial\n",
    "    total_rewards = np.sum(r_arr, axis=1)\n",
    "\n",
    "    for idx, bin_val in enumerate(bins):\n",
    "        # Calculate the angle (cosine similarity) between each dot vector and the current bin vector\n",
    "        cosine_similarity = normalized_dot_arr @ np.array(bin_val)\n",
    "        \n",
    "        # Convert cosine similarity to angular difference\n",
    "        angular_diff = np.arccos(cosine_similarity)\n",
    "\n",
    "        # Check if this bin has the smallest angular difference for each dot vector\n",
    "        is_closest_bin = np.isclose(angular_diff, np.min(angular_diff, axis=1)[:, np.newaxis])\n",
    "        \n",
    "        # Find the top K trials for the current bin\n",
    "        sorted_indices = np.argsort(total_rewards[is_closest_bin[:, 0]])[::-1]\n",
    "        top_trials[:, idx] = np.arange(1000)[is_closest_bin[:, 0]][sorted_indices[:T]]\n",
    "\n",
    "    return top_trials\n",
    "\n",
    "def get_top_rewarded_trials(dot_arr, r_arr, T, N):\n",
    "    norms = np.linalg.norm(dot_arr, axis=2)\n",
    "    normalized_dot_arr = dot_arr / norms[:, :, np.newaxis]\n",
    "    bins = generate_bin_vectors(N)\n",
    "    \n",
    "    # Store the trials for each bin in a list of lists\n",
    "    bin_trials = [[] for _ in range(N)]\n",
    "\n",
    "    for trial_idx, dots in enumerate(normalized_dot_arr):\n",
    "        # Calculate the angle (cosine similarity) between this trial's dot vector and all bins\n",
    "        cosine_similarity = dots @ bins.T\n",
    "        angular_diff = np.arccos(cosine_similarity)\n",
    "        # Get the index of the bin that has the smallest angular difference for this trial\n",
    "        closest_bin_idx = np.argmin(angular_diff, axis=1)[0] \n",
    "        bin_trials[closest_bin_idx].append(trial_idx)\n",
    "        \n",
    "    top_trials = np.empty((T, N), dtype=int)\n",
    "    total_rewards = np.sum(r_arr, axis=1)\n",
    "\n",
    "    for idx, trials in enumerate(bin_trials):\n",
    "        # Sort the trials in this bin based on their rewards and take the top T\n",
    "        sorted_trials = sorted(trials, key=lambda x: total_rewards[x], reverse=True)\n",
    "        top_trials[:, idx] = sorted_trials[:T]\n",
    "\n",
    "    t = has_duplicates(top_trials)\n",
    "    print('has duplicates=',t)\n",
    "\n",
    "    return top_trials\n",
    "\n",
    "def plot_scatter_top_trials(dot_arr, top_trials, bins):\n",
    "    # Calculate the average dot velocity vectors\n",
    "    dot_diffs = np.diff(dot_arr, axis=1)\n",
    "    mean_dot_vel = np.mean(dot_diffs, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    # Draw the dotted lines for bin edges\n",
    "    max_radius = np.max(np.linalg.norm(mean_dot_vel, axis=1))\n",
    "    for bin_vec in bins:\n",
    "        end_x = max_radius * bin_vec[0]\n",
    "        end_y = max_radius * bin_vec[1]\n",
    "        plt.plot([0, end_x], [0, end_y], 'k--')\n",
    "    \n",
    "    # Scatter plot for each bin and compute the center of mass\n",
    "    for idx, trials in enumerate(top_trials.T):\n",
    "        selected_dots = mean_dot_vel[trials]\n",
    "        plt.scatter(selected_dots[:, 0], selected_dots[:, 1], label=f'Bin {idx}', s=20, alpha=0.5)\n",
    "        \n",
    "        # Calculate and plot the center of mass for this bin\n",
    "        centroid_x = np.mean(selected_dots[:, 0])\n",
    "        centroid_y = np.mean(selected_dots[:, 1])\n",
    "        plt.scatter(centroid_x, centroid_y, marker='x', s=200)\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.title(\"Scatter plot of dot velocities for top trials with centroids\")\n",
    "    plt.xlabel(\"X-coordinate\")\n",
    "    plt.ylabel(\"Y-coordinate\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "N = 8\n",
    "T = 100\n",
    "\n",
    "bins = generate_bin_vectors(N)  # Ensure you have the bins for your problem\n",
    "\n",
    "# Generate top trials using the old function\n",
    "top_trials = get_top_rewarded_trials_new(dot_arr, r_arr, T, N)\n",
    "print('top_trials=',top_trials.shape)\n",
    "\n",
    "# Plot the scatter plot using the generated top trials\n",
    "plot_scatter_top_trials(dot_arr, top_trials, bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug (dot vel scatter)\n",
    "\n",
    "%matplotlib inline\n",
    "dot_diffs = np.diff(dot_arr, axis=1)\n",
    "mean_dot_vel = np.mean(dot_diffs, axis=1)\n",
    "norm_dot_vel = np.linalg.norm(mean_dot_vel, axis=1)\n",
    "mean_dot_vel = mean_dot_vel / norm_dot_vel[:,np.newaxis]\n",
    "print('dot_diffs.shape=',dot_diffs.shape)\n",
    "print('mean_dot_vel.shape=',mean_dot_vel.shape)\n",
    "print('norm_dot_vel.shape=',norm_dot_vel.shape)\n",
    "print('mean_dot_vel=',mean_dot_vel.shape)\n",
    "# plot a scatter plot of the dot velocities:\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(mean_dot_vel[:,0], mean_dot_vel[:,1], s=5)\n",
    "plt.title(\"Scatter Plot of Dot Velocities\")\n",
    "plt.xlabel(\"Dot Velocity in X\")\n",
    "plt.ylabel(\"Dot Velocity in Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_rewarded_trials fncs (old)\n",
    "\n",
    "def get_top_rewarded_trials_old(dot_arr, r_arr, T, N):\n",
    "    norms = np.linalg.norm(dot_arr, axis=2)\n",
    "    normalized_dot_arr = dot_arr / norms[:, :, np.newaxis]\n",
    "    bins = generate_bin_vectors(N)\n",
    "    print('bins=',bins)\n",
    "    \n",
    "    # Create an array to store the top K trials for each bin\n",
    "    top_trials = np.empty((T, N), dtype=int)\n",
    "\n",
    "    # Calculate the sum of rewards for each trial\n",
    "    total_rewards = np.sum(r_arr, axis=1)\n",
    "\n",
    "    for idx, bin_val in enumerate(bins):\n",
    "        print('bin_val=',bin_val)\n",
    "        # Calculate the angle (cosine similarity) between each dot vector and the current bin vector\n",
    "        cosine_similarity = normalized_dot_arr @ np.array(bin_val)\n",
    "        \n",
    "        # Convert cosine similarity to angular difference\n",
    "        angular_diff = np.arccos(cosine_similarity)\n",
    "        print('angular_diff=',angular_diff.shape)\n",
    "\n",
    "        # print('min_ang_diff=',np.min(angular_diff, axis=1))\n",
    "\n",
    "        # Check if this bin has the smallest angular difference for each dot vector\n",
    "        is_closest_bin = np.isclose(angular_diff, np.min(angular_diff, axis=1)[:, np.newaxis])\n",
    "        \n",
    "        print(f\"Number of trials closest to bin {idx}: {sum(is_closest_bin[:, 0])}\")\n",
    "        # Find the top K trials for the current bin\n",
    "        sorted_indices = np.argsort(total_rewards[is_closest_bin[:, 0]])[::-1]\n",
    "        top_trials[:, idx] = np.arange(1000)[is_closest_bin[:, 0]][sorted_indices[:T]]\n",
    "\n",
    "    t = has_duplicates(top_trials)\n",
    "    print('has duplicates=',t)\n",
    "\n",
    "    return top_trials\n",
    "\n",
    "def get_top_rewarded_trials(dot_arr, r_arr, T, N):\n",
    "    norms = np.linalg.norm(dot_arr, axis=2)\n",
    "    normalized_dot_arr = dot_arr / norms[:, :, np.newaxis]\n",
    "    bins = generate_bin_vectors(N)\n",
    "    \n",
    "    # Store the trials for each bin in a list of lists\n",
    "    bin_trials = [[] for _ in range(N)]\n",
    "\n",
    "    for trial_idx, dots in enumerate(normalized_dot_arr):\n",
    "        # Calculate the angle (cosine similarity) between this trial's dot vector and all bins\n",
    "        cosine_similarity = dots @ bins.T\n",
    "        angular_diff = np.arccos(cosine_similarity)\n",
    "        # Get the index of the bin that has the smallest angular difference for this trial\n",
    "        closest_bin_idx = np.argmin(angular_diff, axis=1)[0] \n",
    "        bin_trials[closest_bin_idx].append(trial_idx)\n",
    "        \n",
    "    top_trials = np.empty((T, N), dtype=int)\n",
    "    total_rewards = np.sum(r_arr, axis=1)\n",
    "\n",
    "    for idx, trials in enumerate(bin_trials):\n",
    "        # Sort the trials in this bin based on their rewards and take the top T\n",
    "        sorted_trials = sorted(trials, key=lambda x: total_rewards[x], reverse=True)\n",
    "        top_trials[:, idx] = sorted_trials[:T]\n",
    "\n",
    "    t = has_duplicates(top_trials)\n",
    "    print('has duplicates=',t)\n",
    "\n",
    "    return top_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "n = 335\n",
    "\n",
    "def plot_trajectory(data, n):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 3D Plot\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    traj = data[n, :, :3]  # Considering top 3 PCs\n",
    "    length = traj.shape[0]\n",
    "    segments = [traj[i:i+2] for i in range(length-1)]\n",
    "    lc_3d = Line3DCollection(segments, cmap=plt.cm.seismic, norm=plt.Normalize(0, length))\n",
    "    lc_3d.set_array(np.linspace(0, length, len(segments)))\n",
    "    ax1.add_collection(lc_3d)\n",
    "    ax1.set_xlim(traj[:, 0].min(), traj[:, 0].max())\n",
    "    ax1.set_ylim(traj[:, 1].min(), traj[:, 1].max())\n",
    "    ax1.set_zlim(traj[:, 2].min(), traj[:, 2].max())\n",
    "    ax1.set_title('3D Trajectory')\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "\n",
    "    # 2D Plot (assuming the plane of most variation involves the first and second PCs)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    segments_2d = [traj[i:i+2, :2] for i in range(length-1)]\n",
    "    lc_2d = LineCollection(segments_2d, cmap=plt.cm.seismic, norm=plt.Normalize(0, length))\n",
    "    lc_2d.set_array(np.linspace(0, length, len(segments_2d)))\n",
    "    ax2.add_collection(lc_2d)\n",
    "    ax2.autoscale_view()\n",
    "    ax2.set_title('2D Projection')\n",
    "    ax2.set_xlabel('PC1')\n",
    "    ax2.set_ylabel('PC2')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_trajectory(hs_pc_s, n)\n",
    "# plot_trajectory(hv_pc_c, n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax\n",
    "ax1.set_xlabel('PC1', fontsize=fnt, labelpad=-5, rotation=0)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    # ax1.xaxis.pane.fill = False\n",
    "    ax1.set_ylabel('PC2', fontsize=fnt, labelpad=-0, rotation=0)\n",
    "    ax1.set_yticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    ax1.set_zlabel('PC3', fontsize=fnt, labelpad=-5, rotation=90)\n",
    "    ax1.set_zticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(4)\n",
    "    ax1.spines['bottom'].set_linewidth(4)\n",
    "    ax1.grid(False)\n",
    "\n",
    "    ax1.legend(handles=legend_elements, loc=\"upper center\", ncol=1, fontsize=15, frameon=False, handlelength=1.4, columnspacing=0.5, bbox_to_anchor=(0.1, .9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
